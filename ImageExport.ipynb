{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/90031508/183531098-494a5819-7714-4f72-8ff8-d038982eb5f0.png\" alt=\"Water Oracle logo\"/>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "fHqRH40u6ik9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjVV1BFMmg6"
      },
      "source": [
        "# Image export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTRJ_T0vMGoK"
      },
      "source": [
        "This Work is adapted from 'Tensorflow example workflows', \n",
        "https://developers.google.com/earth-engine/guides/tf_examples examples.\n",
        "Copyright 2020 Google LLC. https://www.apache.org/licenses/LICENSE-2.0.\n",
        "\n",
        "Please run this notebook on google colab (pro+)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"https://colab.research.google.com/drive/1kPxIHJQVvns-vKqs0QuYJIZpFRMR78gz?usp=sharing\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/ese-msc-2021/irp-kl121\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
      ],
      "metadata": {
        "id": "QWdYLhlz6len"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfqdSeIUMI4G"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7j7hRueMKS5"
      },
      "source": [
        "## Prerequisites\n",
        "- Google account and logins\n",
        "- Google colab subscription with pro or pro+ is optional but would help with long runtime\n",
        "- Google cloud platform account in order to use google cloud bucket. (Note that you would need sufficient funds to store large amount of models and training data.)\n",
        "- Wandb.ai account which is free of charge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MC4xs43MMIt"
      },
      "source": [
        "## What is this notebook?\n",
        "\n",
        "The main purpose of this notebook is to export image in regions of interest and predict this image and generate the output in as google cloud asset in google earth engine.\n",
        "\n",
        "Initially, the required data is imported from the earth engine repository.\n",
        "\n",
        "- The data used includes:\n",
        "Sentinel-1 Data (10m) \n",
        "https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD?hl=en\n",
        "\n",
        "- USGâ€™s Landsat-8 Collection 1 and Tier 1 (30m) and the cloud is masked\n",
        "https://developers.google.com/earth-engine/guides/landsat\n",
        "- NASADEM: NASA NASADEM Digital Elevation (30m) https://developers.google.com/earth-engine/datasets/catalog/NASA_NASADEM_HGT_001\n",
        "\n",
        "<b>The notebook is splitted into three main objectives:</b>\n",
        "1. Enable user to select any of the models written in the paper to predict the water bodies and export this image\n",
        "\n",
        "2. Case study of the NASADEM data and how it can improve of affect when pairing with Landsat-8 or sentinel-1\n",
        "\n",
        "3. Apply the GlobalWaterNet in order to understand the seasonal effect of water bodies in Tibet\n",
        "\n",
        "4. Apply THWaterNet in order to map southern Thailand Flooding extent in 2016\n",
        "\n",
        "5. GlobalWaterNet to predict 10 different water bodies in different countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heRwSguIMYb3"
      },
      "source": [
        "## Creating Packages\n",
        "\n",
        "Creating the tools packages that will be used throughout the notebook. The package includes\n",
        "- metrics_.py\n",
        "- config.py\n",
        "- preprocessing.py\n",
        "- losses_.py\n",
        "- images.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0HHLJfWMckP",
        "outputId": "70b834a7-2669-4b82-b172-a4f7b5c67efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Aug  3 20:21 sample_data\n",
            "total 0\n",
            "-rw-r--r-- 1 root root 0 Aug  7 23:21 __init__.py\n"
          ]
        }
      ],
      "source": [
        "PACKAGE_PATH = 'tools'\n",
        "\n",
        "!ls -l\n",
        "!mkdir {PACKAGE_PATH}\n",
        "!touch {PACKAGE_PATH}/__init__.py\n",
        "!ls -l {PACKAGE_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {PACKAGE_PATH}/metrics_.py\n",
        "\n",
        "from keras import backend as K\n",
        "import tqdm.notebook as tq\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CONFIG = None\n",
        "\n",
        "__all__ = [\"f1\", \"custom_accuracy\", \"MetricCalculator\", \"MetricCalculator_multiview_2\", \"MetricCalculator_multiview_3\", \"MetricCalculator_NDWI\", \"ndwi_threashold\"]\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    The function is used as tensorflow metrics when training. It takes in the ground truth and the\n",
        "    model predicted result and evaluate the F1 score. This is an experimental function and should not be used as\n",
        "    further model training metric.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : tf.tensor\n",
        "    y_pred : tf.tensor\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    F1 score in keras backend\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This function is flawed because keras calculates the metrics batchwise \n",
        "    which is why F1 metric is removed from keras. To properly calulate the F1 score, we can use the callback function\n",
        "    or manually calculate F1 score after the model has finished training. The latter is chosen and this could be seen\n",
        "    in MetricCalculator, MetricCalculator_multiview_2 and MetricCalculator_multiview_3.\n",
        "  \n",
        "    The reason this function is kept is because the model was initially trained with these metrics and\n",
        "    stored in the google cloud bucket. To retrieve the models these metrics must be passed inorder to retrieve the model.\n",
        "    Since the model is optimize on the loss rather than the metrics, the incorrect metric would not effect the model\n",
        "    training process. The code is obtained/modified from:\n",
        "\n",
        "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "\n",
        "    https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
        "    \"\"\"\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    The function is used as tensorflow metrics when training. It takes in the ground truth and the\n",
        "    model predicted result and evaluate the accuracy score. This is an experimental function and should not be used as\n",
        "    further model training metric.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : tf.tensor\n",
        "    y_pred : tf.tensor\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    accuracy score in keras backend\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This function is modified from the F1 metric above to fit the definition of accuracy. However, tensorflow's\n",
        "    \"categorical_accuracy\" is used instead. The accuracy metric would also be recalculated again in \n",
        "    MetricCalculator, MetricCalculator_multiview_2 and MetricCalculator_multiview_3.\n",
        "  \n",
        "    The reason this function is kept is because the model was initially trained with these metrics and\n",
        "    stored in the google cloud bucket. To retrieve the models these metrics must be passed inorder to retrieve the model.\n",
        "    Since the model is optimize on the loss rather than the metrics, the incorrect metric would not effect the model\n",
        "    training process. The code is obtained/modified from:\n",
        "\n",
        "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "\n",
        "    https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
        "    \"\"\"\n",
        "    # total_data = K.int_shape(y_true) + K.int_shape(y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    true_negatives = K.sum(K.round(K.clip(1 - y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    total_data = - true_positives + true_negatives + possible_positives + predicted_positives\n",
        "    return (true_positives + true_negatives) / (total_data + K.epsilon())\n",
        "\n",
        "\n",
        "\n",
        "def MetricCalculator(model, test_data, total_steps):\n",
        "  \"\"\"\n",
        "  This function takes in the feature stack model loaded from google cloud bucket, the test_data which is the tensor object and\n",
        "  the number of steps and returns the metrics including accuracy, recall, precision and F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : keras.engine.functional.Functional\n",
        "  test_data : RepeatDataset with tf.float32\n",
        "  total_steps : int/float\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  Returns the precision, recall, f1, accuracy metric based on the model performance.\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  This function should be used instead of the F1, custom_accuracy written above. The code is obtained/modified from:\n",
        "\n",
        "  https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "\n",
        "  https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
        "  \"\"\"\n",
        "  pred = []\n",
        "  true = []\n",
        "  pbar = tq.tqdm(total=total_steps)\n",
        "  for steps, data in enumerate(test_data):\n",
        "    # print(f'Number of steps: {steps}', end = \"\\r\")\n",
        "    pbar.update(1)\n",
        "    if steps == total_steps:\n",
        "      break\n",
        "    input = data[0]\n",
        "    y_true = data[1]\n",
        "    y_pred = np.rint(model.predict(input))\n",
        "    y_true = np.reshape(y_true, (256*256,2))\n",
        "    y_pred = np.reshape(y_pred, (256*256,2))\n",
        "    pred.append(y_pred)\n",
        "    true.append(y_true)\n",
        "\n",
        "\n",
        "  f1_macro = f1_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  recall_macro= recall_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  precision_macro = precision_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  accuracy = accuracy_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)))\n",
        "\n",
        "  print(\"precision_macro: \", precision_macro)\n",
        "  print(\"recall_macro: \", recall_macro)\n",
        "  print(\"F1_macro_Score: : \", f1_macro)\n",
        "  print(\"Accuracy: \", accuracy)\n",
        "\n",
        "  return precision_macro, recall_macro, f1_macro, accuracy\n",
        "\n",
        "\n",
        "\n",
        "def MetricCalculator_multiview_2(model, test_data, total_steps):\n",
        "  \"\"\"\n",
        "  This function takes in the multiview-2 model loaded from google cloud bucket, the test_data which is the tensor object and\n",
        "  the number of steps and returns the metrics including accuracy, recall, precision and F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : keras.engine.functional.Functional\n",
        "  test_data : RepeatDataset with tf.float32\n",
        "  total_steps : int/float\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  Returns the precision, recall, f1, accuracy metric based on the model performance.\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  This function should be used instead of the F1, custom_accuracy written above. The code is obtained/modified from:\n",
        "\n",
        "  https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "\n",
        "  https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
        "  \"\"\"\n",
        "  pbar = tq.tqdm(total=total_steps)\n",
        "  pred = []\n",
        "  true = []\n",
        "  for steps, data in enumerate(test_data):\n",
        "    pbar.update(1)\n",
        "    if steps >= total_steps:\n",
        "      break\n",
        "    input = data[0]\n",
        "    x1, x2 = tf.split(input, [len(CONFIG.BANDS1),len(CONFIG.BANDS2)], 3)\n",
        "    y_true = data[1]\n",
        "    y_pred = np.rint(model.predict([x1, x2]))\n",
        "    y_true = np.reshape(y_true, (256*256,2))\n",
        "    y_pred = np.reshape(y_pred, (256*256,2))\n",
        "    pred.append(y_pred)\n",
        "    true.append(y_true)\n",
        "  f1_macro = f1_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  recall_macro= recall_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  precision_macro = precision_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  accuracy = accuracy_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)))\n",
        "\n",
        "  print(\"precision_macro: \", precision_macro)\n",
        "  print(\"recall_macro: \", recall_macro)\n",
        "  print(\"F1_macro_Score: : \", f1_macro)\n",
        "  print(\"Accuracy: \", accuracy)\n",
        "\n",
        "  return precision_macro, recall_macro, f1_macro, accuracy\n",
        "\n",
        "def MetricCalculator_multiview_3(model, test_data, total_steps):\n",
        "  \"\"\"\n",
        "  This function takes in the multiview-3 model loaded from google cloud bucket, the test_data which is the tensor object and\n",
        "  the number of steps and returns the metrics including accuracy, recall, precision and F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : keras.engine.functional.Functional\n",
        "  test_data : RepeatDataset with tf.float32\n",
        "  total_steps : int/float\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  Returns the precision, recall, f1, accuracy metric based on the model performance.\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  This function should be used instead of the F1, custom_accuracy written above. The code is obtained/modified from:\n",
        "\n",
        "  https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "\n",
        "  https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
        "  \"\"\"\n",
        "  pbar = tq.tqdm(total=total_steps)\n",
        "  pred = []\n",
        "  true = []\n",
        "  for steps, data in enumerate(test_data):\n",
        "    pbar.update(1)\n",
        "    if steps >= total_steps:\n",
        "      break\n",
        "    input = data[0]\n",
        "    x1, x2, x3 = tf.split(input, [len(CONFIG.BANDS1),len(CONFIG.BANDS2),len(CONFIG.BANDS3)], 3)\n",
        "    y_true = data[1]\n",
        "    y_pred = np.rint(model.predict([x1, x2, x3]))\n",
        "    y_true = np.reshape(y_true, (256*256,2))\n",
        "    y_pred = np.reshape(y_pred, (256*256,2))\n",
        "    pred.append(y_pred)\n",
        "    true.append(y_true)\n",
        "  f1_macro = f1_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  recall_macro= recall_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  precision_macro = precision_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  accuracy = accuracy_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)))\n",
        "\n",
        "  print(\"precision_macro: \", precision_macro)\n",
        "  print(\"recall_macro: \", recall_macro)\n",
        "  print(\"F1_macro_Score: : \", f1_macro)\n",
        "  print(\"Accuracy: \", accuracy)\n",
        "\n",
        "  return precision_macro, recall_macro, f1_macro, accuracy\n",
        "\n",
        "\n",
        "def ndwi_threashold(B3, B5):\n",
        "  \"\"\"\n",
        "  This function takes in bands 3 and bands 5 from the landsat imagery and returns the tuple prediction of\n",
        "  whether there is water present or not. The threashold is set at 0.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  test_data : RepeatDataset with tf.float32\n",
        "  total_steps : int/float\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple of whether there is water or not\n",
        "  \"\"\"\n",
        "  ndwi = (B3-B5)/(B3+B5)\n",
        "  if ndwi > 0:\n",
        "    return 0, 1\n",
        "  else:\n",
        "    return 1, 0\n",
        "\n",
        "def MetricCalculator_NDWI(test_data, total_steps):\n",
        "  \"\"\"\n",
        "  This function takes in the test_data which is the tensor object and\n",
        "  the number of steps and returns the metrics including accuracy, recall, precision and F1\n",
        "  for NDWI performance.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  test_data : RepeatDataset with tf.float32\n",
        "  total_steps : int/float\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  Returns the precision, recall, f1, accuracy metric based on the NDWI performance\n",
        "  \"\"\"\n",
        "  pred = []\n",
        "  true = []\n",
        "  pbar = tq.tqdm(total=total_steps)\n",
        "  for steps, data in enumerate(test_data):\n",
        "    # print(f'Number of steps: {steps}', end = \"\\r\")\n",
        "    pbar.update(1)\n",
        "    if steps == total_steps:\n",
        "      break\n",
        "    input = data[0]\n",
        "    y_true = data[1]\n",
        "    input = np.reshape(input, (256*256,2))\n",
        "    y_pred = []\n",
        "    for i in range(256*256):\n",
        "      B3, B5 = input[i]\n",
        "      first, second = ndwi_threashold(B3, B5)\n",
        "      y_pred.append([first, second])\n",
        "    y_true = np.reshape(y_true, (256*256,2))\n",
        "    y_pred = np.reshape(y_pred, (256*256,2))\n",
        "    pred.append(y_pred)\n",
        "    true.append(y_true)\n",
        "\n",
        "\n",
        "  f1_macro = f1_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  recall_macro= recall_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  precision_macro = precision_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)), average=\"macro\")\n",
        "  accuracy = accuracy_score(np.reshape(true, (total_steps*65536, 2)), np.reshape(pred, (total_steps*65536, 2)))\n",
        "\n",
        "  print(\"precision_macro: \", precision_macro)\n",
        "  print(\"recall_macro: \", recall_macro)\n",
        "  print(\"F1_macro_Score: : \", f1_macro)\n",
        "  print(\"Accuracy: \", accuracy)\n",
        "\n",
        "  return precision_macro, recall_macro, f1_macro, accuracy"
      ],
      "metadata": {
        "id": "nrPKNepn6qOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0AWFXSMaZ8n",
        "outputId": "b00fa299-50bc-402d-a330-56f447e60a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tools/metrics_.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/config.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from . import metrics_\n",
        "\n",
        "__all__ = [\"configuration\"]\n",
        "\n",
        "class configuration:\n",
        "  \"\"\"\n",
        "  In each experiment, the combinations of satellite's bands that is used to train the neural network is different.\n",
        "  Also the way to train the neural network is also different, whether it is feature stack, multiview learning with two\n",
        "  or three perceptrons. As each experiment has different settings, it is important to store them and reuse this\n",
        "  throughout the project. This class enables user to store the settings and reuse the settings.\n",
        "  \"\"\"\n",
        "  def __init__(self, PROJECT_TITLE, BANDS1, TRAIN_SIZE, EVAL_SIZE, BANDS2=[], BANDS3=[], country=\"TH\", image=None, sam_arr=None, type_=1, LOSS=\"categorical_crossentropy\", EPOCHS=10, BATCH_SIZE = 16, dropout_prob=0.3):\n",
        "    \"\"\"\n",
        "\n",
        "    Initialising/storing the parameters to use later\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    PROJECT_TITLE : string\n",
        "    BANDS1 : list\n",
        "    TRAIN_SIZE : int/float\n",
        "    EVAL_SIZE : int/float\n",
        "    BANDS2 : list\n",
        "    BANDS3 : list\n",
        "    country : string\n",
        "    image : ee.image.Image\n",
        "    sam_arr : ee.image.Image\n",
        "    type : int/float\n",
        "\n",
        "    \"\"\"\n",
        "    if type_ == 1:\n",
        "      self.type_ = \"fs\"\n",
        "    elif type_ == 2:\n",
        "      self.type_ = \"m2\"\n",
        "    elif type_ == 3:\n",
        "      self.type_ = \"m3\"\n",
        "    else:\n",
        "      self.type_ = None\n",
        "    self.country = country\n",
        "    self.PROJECT_TITLE = PROJECT_TITLE\n",
        "    self.BANDS1 = BANDS1\n",
        "    self.BANDS2 = BANDS2\n",
        "    self.BANDS3 = BANDS3\n",
        "    self.BUCKET = \"geebucketwater\"\n",
        "    self.FOLDER = f'{self.type_}_{self.country}_Cnn_{self.PROJECT_TITLE}'\n",
        "    self.TRAIN_SIZE = TRAIN_SIZE\n",
        "    self.EVAL_SIZE = EVAL_SIZE\n",
        "    self.BUCKET = \"geebucketwater\"\n",
        "    self.TRAINING_BASE = f'training_patches'\n",
        "    self.EVAL_BASE = f'eval_patches'\n",
        "    self.TEST_BASE = f'test_patches'\n",
        "    self.RESPONSE = 'water'\n",
        "    self.BANDS = BANDS1 + BANDS2 + BANDS3 \n",
        "    self.FEATURES = BANDS1 + BANDS2 + BANDS3 + [self.RESPONSE]\n",
        "    # Specify the size and shape of patches expected by the model.\n",
        "    self.KERNEL_SIZE = 256\n",
        "    self.KERNEL_SHAPE = [self.KERNEL_SIZE, self.KERNEL_SIZE]\n",
        "    self.COLUMNS = [\n",
        "      tf.io.FixedLenFeature(shape=self.KERNEL_SHAPE, dtype=tf.float32) for k in self.FEATURES\n",
        "    ]\n",
        "    self.FEATURES_DICT = dict(zip(self.FEATURES, self.COLUMNS))\n",
        "    # Specify model training parameters.\n",
        "    self.BATCH_SIZE = BATCH_SIZE\n",
        "    self.EPOCHS = EPOCHS\n",
        "    self.BUFFER_SIZE = 2000\n",
        "    self.OPTIMIZER = 'adam'\n",
        "    self.LOSS = LOSS\n",
        "    self.dropout_prob = dropout_prob\n",
        "    self.METRICS = ['AUC', \"categorical_accuracy\", metrics_.f1]\n",
        "    self.image = image\n",
        "    self.sam_arr = sam_arr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fByvYYdMdn3",
        "outputId": "bd920177-2adc-4067-ac3f-851710137584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tools/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/preprocessing.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import ee\n",
        "\n",
        "__all__ = [\"Preprocessor\", \"maskL8sr\", \"EnsureTwodigit\", \"GenSeasonalDatesMonthly\"]\n",
        "\n",
        "class Preprocessor:\n",
        "  \"\"\"\n",
        "  Class that preprocessese and returns the training, evaluation and testing data from google cloud bucket\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    self.config = config\n",
        "\n",
        "  def parse_tfrecord(self, example_proto):\n",
        "    \"\"\"\n",
        "    The parsing function Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  \n",
        "    Parameters\n",
        "    ----------\n",
        "    example_proto: a serialized Example\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    return tf.io.parse_single_example(example_proto, self.config.FEATURES_DICT)\n",
        "\n",
        "\n",
        "  def to_tuple(self, inputs):\n",
        "    \"\"\"\n",
        "    Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tuple of (inputs, outputs).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    inputsList = [inputs.get(key) for key in self.config.FEATURES]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    # Convert from CHW to HWC\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked[:,:,:len(self.config.BANDS)], tf.reshape(tf.one_hot(tf.cast(stacked[:,:,len(self.config.BANDS):], tf.int32), depth=2),[256,256,2])\n",
        "\n",
        "\n",
        "  def get_dataset(self, pattern):\n",
        "    \"\"\"\n",
        "    Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "    Get all the files matching the pattern, parse and convert to tuple.\n",
        "    Parameters\n",
        "    ----------\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tf.data.Dataset\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    try:\n",
        "      glob = tf.io.gfile.glob(pattern)\n",
        "    except:\n",
        "      # print(\"the bucket you specified doesn't exist\")\n",
        "      return \"the bucket you specified doesn't exist\"\n",
        "    # glob = tf.io.gfile.glob(pattern)\n",
        "    if glob == []:\n",
        "      return \"the path you specified doesn't have the data\"\n",
        "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "    dataset = dataset.map(self.parse_tfrecord, num_parallel_calls=5)\n",
        "    dataset = dataset.map(self.to_tuple, num_parallel_calls=5)\n",
        "    return dataset\n",
        "\n",
        "  def get_training_dataset(self, location):\n",
        "    \"\"\"\n",
        "    Get the preprocessed training dataset\n",
        "    Parameters\n",
        "    ----------\n",
        "    location: string\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tf.data.Dataset of training data.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + self.config.BUCKET + '/' + location + \"training_patches_\" + '*'\n",
        "    # print(glob)\n",
        "    dataset = self.get_dataset(glob)\n",
        "    dataset = dataset.shuffle(self.config.BUFFER_SIZE).batch(self.config.BATCH_SIZE).repeat()\n",
        "    return dataset\n",
        "\n",
        "  def get_training_dataset_for_testing(self, location):\n",
        "    \"\"\"\n",
        "    Get the preprocessed training dataset for testing\n",
        "    Parameters\n",
        "    ----------\n",
        "    location: string\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tf.data.Dataset of training data.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + self.config.BUCKET + '/' + location + \"training_patches_\" + '*'\n",
        "    # print(glob)\n",
        "    dataset = self.get_dataset(glob)\n",
        "    if type(dataset) == str:\n",
        "      return dataset\n",
        "    dataset = dataset.batch(1).repeat()\n",
        "    return dataset\n",
        "\n",
        "  def get_eval_dataset(self, location):\n",
        "    \"\"\"\n",
        "    Get the preprocessed evaluation dataset\n",
        "    Parameters\n",
        "    ----------\n",
        "    location: string\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tf.data.Dataset of evaluation data.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + self.config.BUCKET + '/' + location + \"eval_patches_\" + '*'\n",
        "    # print(glob)\n",
        "    dataset = self.get_dataset(glob)\n",
        "    if type(dataset) == str:\n",
        "      return dataset\n",
        "    dataset = dataset.batch(1).repeat()\n",
        "    return dataset\n",
        "\n",
        "  # print(iter(evaluation.take(1)).next())\n",
        "\n",
        "  def get_test_dataset(self, location, test_base):\n",
        "    \"\"\"\n",
        "    Get the preprocessed testing dataset\n",
        "    Parameters\n",
        "    ----------\n",
        "    location: string\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tf.data.Dataset of testing data.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + self.config.BUCKET + '/' + location + test_base + '*'\n",
        "    # print(glob)\n",
        "    dataset = self.get_dataset(glob)\n",
        "    if type(dataset) == str:\n",
        "      return dataset\n",
        "    dataset = dataset.batch(1).repeat()\n",
        "    return dataset\n",
        "\n",
        "def maskL8sr(image):\n",
        "    \"\"\"\n",
        "    Get the landsat-8 image and returned a cloud masked image\n",
        "    ----------\n",
        "    image: ee.image.Image\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A maksed landsat-8 ee.image.Image\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "    cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "    cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "    qa = image.select('pixel_qa')\n",
        "    mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "      qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "    return image.updateMask(mask).select(BANDS).divide(10000)\n",
        "\n",
        "\n",
        "def EnsureTwodigit(number):\n",
        "  \"\"\"\n",
        "  Transform the input month into string in the\n",
        "  correct format for date and time.\n",
        "  ----------\n",
        "  number: int\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  months in string.\n",
        "\n",
        "  \"\"\"\n",
        "  if number > 12:\n",
        "    return str(12)\n",
        "  if number < 10:\n",
        "    return \"0\"+str(number)\n",
        "  else:\n",
        "    return str(number)\n",
        "\n",
        "def GenSeasonalDatesMonthly(start, end, month_frequency = 3):\n",
        "  \"\"\"\n",
        "  Given two dictionary containing the key month and year,\n",
        "  return two arrays that contains the time between the \n",
        "  interval of start and end.\n",
        "  ----------\n",
        "  start: dict\n",
        "  end: dict\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  Two arrays containing the time elapsed between start and end\n",
        "\n",
        "  \"\"\"\n",
        "  diff_year = end[\"year\"] - start[\"year\"]\n",
        "  diff_month = end[\"month\"] - start[\"month\"]\n",
        "  starts = []\n",
        "  ends = []\n",
        "  first_data = str(start[\"year\"]) + \"-\" + EnsureTwodigit(start[\"month\"]) + \"-01\"\n",
        "  if diff_year > 0:\n",
        "    return \"please insert the same year\"\n",
        "  else:\n",
        "    for i in range(round(diff_month/month_frequency)):\n",
        "      first_data = str(start[\"year\"]) + \"-\" + EnsureTwodigit(start[\"month\"] + month_frequency * i) + \"-01\"\n",
        "      second_data = str(start[\"year\"]) + \"-\" + EnsureTwodigit(start[\"month\"] + month_frequency * i + month_frequency) + \"-01\"\n",
        "      starts.append(first_data)\n",
        "      ends.append(second_data)\n",
        "  return starts, ends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJJi4dl3Mhin",
        "outputId": "3a1d0f0c-f88c-45f8-ea17-79fbc3f2e53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tools/preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/losses_.py\n",
        "\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "\n",
        "__all__ = [\"dice_coef\", \"dice_p_cc\"]\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    \"\"\"\n",
        "    Recieve the true and predicted tensor and return the resulting dice loss\n",
        "    to prevent overfitting.\n",
        "    ----------\n",
        "    y_true: tf.float32\n",
        "    y_pred: tf.float32\n",
        "    smooth: int/float\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tf.float32 with same dimension as input tf.float32\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://www.kaggle.com/code/kmader/u-net-with-dice-and-augmentation/notebook\n",
        "    \"\"\"\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
        "\n",
        "\n",
        "def dice_p_cc(in_gt, in_pred):\n",
        "    \"\"\"\n",
        "    Recieve the true and predicted tensor and return the resulting categorical dice loss\n",
        "    ----------\n",
        "    in_gt: tf.float32\n",
        "    in_pred: tf.float32\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tf.float32 with same dimension as input tf.float32\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://www.kaggle.com/code/kmader/u-net-with-dice-and-augmentation/notebook\n",
        "    \"\"\"\n",
        "    return categorical_crossentropy(in_gt, in_pred) - K.log(dice_coef(in_gt, in_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nx99K39aIv0",
        "outputId": "9a43060c-7c1a-4786-987b-de4f06bbde51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tools/losses_.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/images.py\n",
        "\n",
        "import json\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "import tensorflow as tf\n",
        "import ee\n",
        "import subprocess\n",
        "import ast\n",
        "\n",
        "\n",
        "__all__ = [\"doExport\", \"predictionSingleinput\", \"predictionMultipleinput\", \"predictionMultipleinput_3\", \\\n",
        "           \"uploadToGEEAsset\", \"LoadImage\", \"doPrediction_featurestack\", \\\n",
        "            \"doPrediction_multiview_2\", \"doPrediction_multiview_3\"]\n",
        "\n",
        "\n",
        "def doExport(out_image_base, kernel_buffer, region, setting, extra_folder= \"\"):\n",
        "  \"\"\"\n",
        "  Export the image with features and area of interest\n",
        "  to google cloud bucket. The function doesn't exit until\n",
        "  the task is complete. The optional extra_folder arguement lets you put\n",
        "  the exported tf.record.gz in a folder\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  out_image_base : string\n",
        "  kernel_buffer : list\n",
        "  region : ee.Geometry.BBox\n",
        "  setting : dict\n",
        "  extra_folder : string\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  if extra_folder == \"\":\n",
        "    fileNamePrefix_ = setting.FOLDER + '/' + out_image_base\n",
        "  else:\n",
        "    fileNamePrefix_ = setting.FOLDER + '/' + extra_folder + '/' + out_image_base\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = setting.image.select(setting.BANDS),\n",
        "    description = out_image_base,\n",
        "    bucket = setting.BUCKET,\n",
        "    fileNamePrefix = fileNamePrefix_,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': setting.KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to Cloud Storage...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')\n",
        "\n",
        "\n",
        "def LoadImage(out_image_base, user_folder, kernel_buffer, setting, extra_folder = \"\"):\n",
        "  \"\"\"\n",
        "  Load the image from the google cloud bucket and preprocess the image for prediction\n",
        "  and provide crutial information for exporting to GEE asset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  out_image_base : string\n",
        "  user_folder : string\n",
        "  kernel_buffer : list\n",
        "  setting : dict\n",
        "  extra_folder : string\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  imageDataset as tensor data for prediction, patches as int, x_buffer as int, y_buffer as int, jsonFile as string\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # Get a list of all the files in the output bucket.\n",
        "  if extra_folder == \"\":\n",
        "    process = subprocess.run([\"gsutil\", \"ls\", f'gs://{setting.BUCKET}/{setting.FOLDER}'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    filesList = process.stdout.split('\\n')\n",
        "  else:\n",
        "    process = subprocess.run([\"gsutil\", \"ls\", f'gs://{setting.BUCKET}/{setting.FOLDER}/{extra_folder}'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    filesList = process.stdout.split('\\n')\n",
        "\n",
        "  print(filesList)\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "  print(exportFilesList)\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "  if jsonFile == None:\n",
        "    return \"image path doesn't exist\"\n",
        "\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  process = subprocess.run([\"gsutil\", \"cat\", f'{jsonFile}'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "  jsonText = process.stdout\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  # mixer = json.loads(jsonText.nlstr)\n",
        "  mixer = ast.literal_eval(jsonText)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "\n",
        "  buffered_shape = [\n",
        "      setting.KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      setting.KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) \n",
        "      for k in setting.BANDS\n",
        "  ]\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    \"\"\"\n",
        "    The parsing function Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    example_proto: a serialized Example\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    \"\"\"\n",
        "    Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    A tuple of (inputs, outputs).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The code is obtained/modified from:\n",
        "\n",
        "    https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "    \"\"\"\n",
        "    inputsList = [inputs.get(key) for key in setting.BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "  imageFeaturesDict = dict(zip(setting.BANDS, imageColumns))\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "  return imageDataset, patches, x_buffer, y_buffer, jsonFile\n",
        "\n",
        "def predictionSingleinput(model, imageDataset, patches):\n",
        "  \"\"\"\n",
        "  Given the model, and image for prediction, predict the image\n",
        "  with feature stack U-Net\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : model : keras.engine.functional.Functional\n",
        "  imageDataset : tf.data.Dataset of training data (BatchDataset)\n",
        "  patches : int\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  A tf.float32 with same dimension as imageDataset\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  print('Running predictions...')\n",
        "  predictions = model.predict(imageDataset, steps=patches, verbose=1)\n",
        "  return predictions\n",
        "\n",
        "def predictionMultipleinput(model, imageDataset, patches, setting):\n",
        "  \"\"\"\n",
        "  Given the model, and image for prediction, predict the image\n",
        "  with Multi-view learning U-Net\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : model : keras.engine.functional.Functional\n",
        "  imageDataset : tf.data.Dataset of training data (BatchDataset)\n",
        "  patches : int\n",
        "  setting : dict\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  A tf.float32 with same dimension as imageDataset\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  print('Running predictions...')\n",
        "  predictions = []\n",
        "  pbar = tq.tqdm(total=patches)\n",
        "  for data in imageDataset:\n",
        "    pbar.update(1)\n",
        "    x1, x2 = tf.split(data, [len(setting.BANDS1), len(setting.BANDS2)], 3)\n",
        "    predictions.append(model.predict([x1, x2], verbose=0))\n",
        "  return predictions\n",
        "\n",
        "def predictionMultipleinput_3(model, imageDataset, patches, setting):\n",
        "  \"\"\"\n",
        "  Given the model, and image for prediction, predict the image\n",
        "  with Multi-view learning U-Net with 3 inputs\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : model : keras.engine.functional.Functional\n",
        "  imageDataset : tf.data.Dataset of training data (BatchDataset)\n",
        "  patches : int\n",
        "  setting : dict\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  A tf.float32 with same dimension as imageDataset\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  print('Running predictions...')\n",
        "  predictions = []\n",
        "  pbar = tq.tqdm(total=patches)\n",
        "  for data in imageDataset:\n",
        "    pbar.update(1)\n",
        "    x1, x2, x3 = tf.split(data, [len(setting.BANDS1), len(setting.BANDS2), len(setting.BANDS3)], 3)\n",
        "    predictions.append(model.predict([x1, x2, x3], verbose=0))\n",
        "  return predictions\n",
        "\n",
        "def uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, multiview=False, user_folder='users/mewchayutaphong'):\n",
        "  \"\"\"\n",
        "  Given the predictions, exported file location other information\n",
        "  on the image to be exported, return the required information\n",
        "  in order to export the predicted image to the GEE asset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_buffer : int\n",
        "  y_buffer : int\n",
        "  predictions : tf.data.Dataset of training data (BatchDataset)\n",
        "  out_image_base : string\n",
        "  jsonFile : string\n",
        "  suffix : string\n",
        "  setting : dict\n",
        "  user_folder : string\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  A out_image_asset a location to the output GEE asset folder\n",
        "  as a string, out_image_file of the prediction as TFRecord and the\n",
        "  corresponding json file.\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + setting.BUCKET + '/' + setting.FOLDER + '/' + out_image_base + '.TFRecord'\n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    if multiview == True:\n",
        "      predictionPatch = predictionPatch[0]\n",
        "    print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+setting.KERNEL_SIZE, y_buffer:y_buffer+setting.KERNEL_SIZE]\n",
        "    predictionPatch = np.argmax(predictionPatch, -1)\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'prediction': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=predictionPatch.flatten()))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  writer.close()\n",
        " \n",
        "  # Start the upload.\n",
        "  out_image_asset = user_folder + '/' + out_image_base + suffix\n",
        "  # !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "  return out_image_asset, out_image_file, jsonFile\n",
        "\n",
        "\n",
        "def doPrediction_featurestack(out_image_base, user_folder, kernel_buffer, model, suffix, setting, extra_folder=\"\"):\n",
        "  \"\"\"\n",
        "  Putting all the image functions together. Load the Image, predict the output with featurestack U-Net, return information\n",
        "  ready to be exported to GEE asset\n",
        "  ----------\n",
        "  pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  A out_image_asset a location to the output GEE asset folder\n",
        "  as a string, out_image_file of the prediction as TFRecord and the\n",
        "  corresponding json file.\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  output_load_image = LoadImage(out_image_base, user_folder, kernel_buffer, setting, extra_folder)\n",
        "  if type(output_load_image) == str:\n",
        "    return \"wrong file location\"\n",
        "  imageDataset, patches, x_buffer, y_buffer, jsonFile = output_load_image\n",
        "  predictions = predictionSingleinput(model, imageDataset, patches)\n",
        "  out_image_asset, out_image_file, jsonFile = uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, False)\n",
        "  return out_image_asset, out_image_file, jsonFile\n",
        "\n",
        "def doPrediction_multiview_2(out_image_base, user_folder, kernel_buffer, model, suffix, setting, extra_folder=\"\"):\n",
        "  \"\"\"\n",
        "  Putting all the image functions together. Load the Image, predict the output with \n",
        "  Multi-view U-Net (2 inputs), return information ready to be exported to GEE asset\n",
        "  ----------\n",
        "  pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  A out_image_asset a location to the output GEE asset folder\n",
        "  as a string, out_image_file of the prediction as TFRecord and the\n",
        "  corresponding json file.\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  output_load_image = LoadImage(out_image_base, user_folder, kernel_buffer, setting, extra_folder)\n",
        "  if type(output_load_image) == str:\n",
        "    return \"wrong file location\"\n",
        "  imageDataset, patches, x_buffer, y_buffer, jsonFile = output_load_image\n",
        "  predictions = predictionMultipleinput(model, imageDataset, patches, setting)\n",
        "  out_image_asset, out_image_file, jsonFile = uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, True)\n",
        "  return out_image_asset, out_image_file, jsonFile\n",
        "\n",
        "def doPrediction_multiview_3(out_image_base, user_folder, kernel_buffer, model, suffix, setting, extra_folder=\"\"):\n",
        "  \"\"\"\n",
        "  Putting all the image functions together. Load the Image, predict the output with \n",
        "  Multi-view U-Net (3 inputs), return information ready to be exported to GEE asset\n",
        "  ----------\n",
        "  pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  A out_image_asset a location to the output GEE asset folder\n",
        "  as a string, out_image_file of the prediction as TFRecord and the\n",
        "  corresponding json file.\n",
        "\n",
        "  Notes\n",
        "  -----\n",
        "  The code is obtained/modified from:\n",
        "\n",
        "  https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\n",
        "  \"\"\"\n",
        "  output_load_image = LoadImage(out_image_base, user_folder, kernel_buffer, setting, extra_folder)\n",
        "  if type(output_load_image) == str:\n",
        "    return \"wrong file location\"\n",
        "  imageDataset, patches, x_buffer, y_buffer, jsonFile = output_load_image\n",
        "  predictions = predictionMultipleinput_3(model, imageDataset, patches, setting)\n",
        "  out_image_asset, out_image_file, jsonFile = uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, True)\n",
        "  return out_image_asset, out_image_file, jsonFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmmHsbDvMr44"
      },
      "source": [
        "## Authentication\n",
        "\n",
        "Authentication with google colab, earth engine api and google cloud bucket is required before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdkmiUoaMrW-",
        "outputId": "e9b0de39-5168-4d08-d434-f319bd521f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=T7CFfqdjSz3F0AtP5Yy2PFiuUyV7U2OY_vVU9MhGbCA&tc=Xfy8oshXohRLUFOc5DxOXg1XcmouU2F6kNLGmLmJ5AY&cc=dR0CLgL9jVm_b08qME-XzRv0gHLCfhUrQNa61xY2nng\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1AdQt8qgGlT3f1eiXCtfV7sa8U__0sY40ALqV3Jh6ScWd046qD2PDZHSP0-w\n",
            "\n",
            "Successfully saved authorization token.\n",
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "project_id = 'coastal-cell-299117'\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBZC9jMaMvZ5"
      },
      "outputs": [],
      "source": [
        "from importlib import reload\n",
        "import tensorflow as tf\n",
        "import folium\n",
        "from pprint import pprint\n",
        "# reload(images) # Uncomment this line to rerun the modified packages\n",
        "from tools import preprocessing, config, metrics_, losses_, images\n",
        "from tensorflow.keras import losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding"
      ],
      "metadata": {
        "id": "n33dRBvsiB3f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsOOpIBoM4J1"
      },
      "source": [
        "\n",
        "\n",
        "## Objective 1: Predict water bodies at any location by selecting a model from GCB\n",
        "\n",
        "- There are a total of 154 models stored in the GCB and the user can select their pretrained model from GCB. \n",
        "- User select a region of interest of where they want to predict the image which gets exported to GCB\n",
        "- The image from GCB is imported into to do prediction\n",
        "- Predicted image is exported to GEE asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi43AvReMy8W"
      },
      "outputs": [],
      "source": [
        "start_date = '2018-01-01'\n",
        "end_date = '2018-02-01'\n",
        "# start_date = '2018-01-01'\n",
        "# end_date = '2018-02-01'\n",
        "\n",
        "# Sentinel-1 Data (10m)\n",
        "S1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filterDate(start_date,end_date) \\\n",
        "\n",
        "S1A = S1.median()\n",
        "S1 = S1.select('VV', 'VH').median()\n",
        "\n",
        "# USGâ€™s Landsat-8 Collection 1 and Tier 1 (30m)\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterDate(start_date,end_date)\n",
        "\n",
        "# Cloud masking function.\n",
        "L8SR = l8sr.map(preprocessing.maskL8sr).median()\n",
        "\n",
        "# NASADEM: NASA NASADEM Digital Elevation (30m)\n",
        "elevation = ee.Image('NASA/NASADEM_HGT/001').select('elevation');\n",
        "slope = ee.Terrain.slope(elevation);\n",
        "aspect = ee.Terrain.aspect(elevation);\n",
        "\n",
        "# JRC-Monthly Water history (30m)\n",
        "waterdata = ee.ImageCollection('JRC/GSW1_3/MonthlyHistory').filterDate(start_date, end_date).median()\n",
        "watermask = waterdata.select(\"water\")\n",
        "mask = watermask.gt(0) # masking out \"no data\" region\n",
        "maskedComposite = waterdata.updateMask(mask).subtract(1) # Shifting the labels to make it binary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClOi0xwZNQV9"
      },
      "source": [
        "### Setting up Config - to train in Thailand\n",
        "\n",
        "In total there are:\n",
        "- 32 feature stack deep learning experiments\n",
        "- 37 Multiview with 2 input experiments\n",
        "- 8 Multiview with 3 input experiments\n",
        "\n",
        "Each experiment has a different configuration for example, different experiment name, bands for each input layer. Hence, a configuration is neccesary. There is also configuration for training globally and is found in Preprocessing_and_export_global.ipynb notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiGAZ1GFNN58"
      },
      "outputs": [],
      "source": [
        "configs_fs = {}\n",
        "train_size = 240*3\n",
        "eval_size = 240*2\n",
        "\n",
        "# Feature stack's 32 experiments\n",
        "\n",
        "configs_fs[\"S1A_el_sl_as\"] = config.configuration(\"S1A_el_sl_as\", [\"VV\", \"VH\", \"angle\", \"elevation\", \"slope\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"S1A_el\"] = config.configuration(\"S1A_el\", [\"VV\", \"VH\", \"angle\", \"elevation\"], train_size, eval_size)\n",
        "configs_fs[\"S1A_sl\"] = config.configuration(\"S1A_sl\", [\"VV\", \"VH\", \"angle\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"S1A_as\"] = config.configuration(\"S1A_as\", [\"VV\", \"VH\", \"angle\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"S1A_sl_as\"] = config.configuration(\"S1A_sl_as\", [\"VV\", \"VH\", \"angle\", \"slope\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"S1A_el_sl\"] = config.configuration(\"S1A_el_sl\", [\"VV\", \"VH\", \"angle\", \"elevation\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"S1A_el_as\"] = config.configuration(\"S1A_el_as\", [\"VV\", \"VH\", \"angle\", \"elevation\", \"aspect\"], train_size, eval_size)\n",
        "\n",
        "configs_fs[\"S1_el_sl_as\"] = config.configuration(\"S1_el_sl_as\", [\"VV\", \"VH\", \"elevation\", \"slope\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"S1_el\"] = config.configuration(\"S1_el\", [\"VV\", \"VH\", \"elevation\"], train_size, eval_size)\n",
        "configs_fs[\"S1_sl\"] = config.configuration(\"S1_sl\", [\"VV\", \"VH\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"S1_as\"] = config.configuration(\"S1_as\", [\"VV\", \"VH\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"S1_sl_as\"] = config.configuration(\"S1_sl_as\", [\"VV\", \"VH\", \"slope\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"S1_el_sl\"] = config.configuration(\"S1_el_sl\", [\"VV\", \"VH\", \"elevation\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"S1_el_as\"] = config.configuration(\"S1_el_as\", [\"VV\", \"VH\", \"elevation\", \"aspect\"], train_size, eval_size)\n",
        "\n",
        "configs_fs[\"L8SR_el_sl_as\"] = config.configuration(\"L8SR_el_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\", \"slope\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_el\"] = config.configuration(\"L8SR_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_sl\"] = config.configuration(\"L8SR_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_as\"] = config.configuration(\"L8SR_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_sl_as\"] = config.configuration(\"L8SR_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"slope\", \"aspect\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_el_sl\"] = config.configuration(\"L8SR_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_el_as\"] = config.configuration(\"L8SR_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\", \"aspect\"], train_size, eval_size)\n",
        "\n",
        "configs_fs[\"L8SR_S1_el\"] = config.configuration(\"L8SR_S1_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"elevation\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_S1_sl\"] = config.configuration(\"L8SR_S1_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_S1_sl_el_as\"] = config.configuration(\"L8SR_S1_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"slope\",\"elevation\", \"aspect\"], train_size, eval_size)\n",
        "\n",
        "configs_fs[\"L8SR_S1A_el\"] = config.configuration(\"L8SR_S1A_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\", \"elevation\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_S1A_sl\"] = config.configuration(\"L8SR_S1A_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\", \"slope\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_S1A_sl_el_as\"] = config.configuration(\"L8SR_S1A_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\", \"slope\",\"elevation\", \"aspect\"], train_size, eval_size)\n",
        "\n",
        "configs_fs[\"L8SR\"] = config.configuration(\"L8SR\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size)\n",
        "configs_fs[\"S1\"] = config.configuration(\"S1\", [\"VV\", \"VH\"], train_size, eval_size)\n",
        "configs_fs[\"S1A\"] = config.configuration(\"S1A\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_S1\"] = config.configuration(\"L8SR_S1\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\"], train_size, eval_size)\n",
        "configs_fs[\"L8SR_S1A\"] = config.configuration(\"L8SR_S1A\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\"], train_size, eval_size)\n",
        "\n",
        "# Multi-view with 2 inputs learning's 37 experiments\n",
        "\n",
        "configs_multi = {}\n",
        "\n",
        "configs_multi[\"S1A_el_sl_as\"] = config.configuration(\"S1A_el_sl_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\", \"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"S1A_el\"] = config.configuration(\"S1A_el\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\"], type_=2)\n",
        "configs_multi[\"S1A_sl\"] = config.configuration(\"S1A_sl\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"slope\"], type_=2)\n",
        "configs_multi[\"S1A_as\"] = config.configuration(\"S1A_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"aspect\"], type_=2)\n",
        "configs_multi[\"S1A_sl_as\"] = config.configuration(\"S1A_sl_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"S1A_el_sl\"] = config.configuration(\"S1A_el_sl\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\", \"slope\"], type_=2)\n",
        "configs_multi[\"S1A_el_as\"] = config.configuration(\"S1A_el_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\", \"aspect\"], type_=2)\n",
        "\n",
        "configs_multi[\"S1_el_sl_as\"] = config.configuration(\"S1_el_sl_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\", \"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"S1_el\"] = config.configuration(\"S1_el\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\"], type_=2)\n",
        "configs_multi[\"S1_sl\"] = config.configuration(\"S1_sl\", [\"VV\", \"VH\"], train_size, eval_size, [\"slope\"], type_=2)\n",
        "configs_multi[\"S1_as\"] = config.configuration(\"S1_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"aspect\"], type_=2)\n",
        "configs_multi[\"S1_sl_as\"] = config.configuration(\"S1_sl_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"S1_el_sl\"] = config.configuration(\"S1_el_sl\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\", \"slope\"], type_=2)\n",
        "configs_multi[\"S1_el_as\"] = config.configuration(\"S1_el_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\", \"aspect\"], type_=2)\n",
        "\n",
        "configs_multi[\"L8SR_el_sl_as\"] = config.configuration(\"L8SR_el_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\", \"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_el\"] = config.configuration(\"L8SR_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\"], type_=2)\n",
        "configs_multi[\"L8SR_sl\"] = config.configuration(\"L8SR_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"slope\"], type_=2)\n",
        "configs_multi[\"L8SR_as\"] = config.configuration(\"L8SR_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_sl_as\"] = config.configuration(\"L8SR_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_el_sl\"] = config.configuration(\"L8SR_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\", \"slope\"], type_=2)\n",
        "configs_multi[\"L8SR_el_as\"] = config.configuration(\"L8SR_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\", \"aspect\"], type_=2)\n",
        "\n",
        "\n",
        "configs_multi[\"L8SR_S1_as\"] = config.configuration(\"L8SR_S1_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_S1_el\"] = config.configuration(\"L8SR_S1_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"elevation\"], type_=2)\n",
        "configs_multi[\"L8SR_S1_sl\"] = config.configuration(\"L8SR_S1_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"slope\"], type_=2)\n",
        "\n",
        "configs_multi[\"L8SR_S1_sl_as\"] = config.configuration(\"L8SR_S1_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_S1_el_sl\"] = config.configuration(\"L8SR_S1_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"elevation\", \"slope\"], type_=2)\n",
        "configs_multi[\"L8SR_S1_el_as\"] = config.configuration(\"L8SR_S1_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"elevation\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_S1_sl_el_as\"] = config.configuration(\"L8SR_S1_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"slope\",\"elevation\", \"aspect\"], type_=2)\n",
        "\n",
        "configs_multi[\"L8SR_S1A_as\"] = config.configuration(\"L8SR_S1A_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_S1A_el\"] = config.configuration(\"L8SR_S1A_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"elevation\"], type_=2)\n",
        "configs_multi[\"L8SR_S1A_sl\"] = config.configuration(\"L8SR_S1A_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"slope\"], type_=2)\n",
        "\n",
        "configs_multi[\"L8SR_S1A_sl_as\"] = config.configuration(\"L8SR_S1A_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"slope\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_S1A_el_sl\"] = config.configuration(\"L8SR_S1A_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"elevation\", \"slope\"], type_=2)\n",
        "configs_multi[\"L8SR_S1A_el_as\"] = config.configuration(\"L8SR_S1A_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"elevation\", \"aspect\"], type_=2)\n",
        "configs_multi[\"L8SR_S1A_sl_el_as\"] = config.configuration(\"L8SR_S1A_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"slope\",\"elevation\", \"aspect\"], type_=2)\n",
        "\n",
        "configs_multi[\"L8SR_S1\"] = config.configuration(\"L8SR_S1\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], type_=2)\n",
        "configs_multi[\"L8SR_S1A\"] = config.configuration(\"L8SR_S1A\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], type_=2)\n",
        "\n",
        "# Multi-view with 3 inputs learning's 8 experiments\n",
        "\n",
        "configs_multi_3 = {}\n",
        "\n",
        "configs_multi_3[\"L8SR_S1_as3\"] = config.configuration(\"L8SR_S1_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"aspect\"], type_=3)\n",
        "configs_multi_3[\"L8SR_S1_el3\"] = config.configuration(\"L8SR_S1_el3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"elevation\"], type_=3)\n",
        "configs_multi_3[\"L8SR_S1_sl3\"] = config.configuration(\"L8SR_S1_sl3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"slope\"], type_=3)\n",
        "configs_multi_3[\"L8SR_S1_sl_el_as3\"] = config.configuration(\"L8SR_S1_sl_el_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"slope\",\"elevation\", \"aspect\"], type_=3)\n",
        "\n",
        "configs_multi_3[\"L8SR_S1A_as3\"] = config.configuration(\"L8SR_S1A_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"aspect\"], type_=3)\n",
        "configs_multi_3[\"L8SR_S1A_el3\"] = config.configuration(\"L8SR_S1A_el3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"elevation\"], type_=3)\n",
        "configs_multi_3[\"L8SR_S1A_sl3\"] = config.configuration(\"L8SR_S1A_sl3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"slope\"], type_=3)\n",
        "configs_multi_3[\"L8SR_S1A_sl_el_as3\"] = config.configuration(\"L8SR_S1A_sl_el_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"slope\",\"elevation\", \"aspect\"], type_=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRHipiu_NGdi"
      },
      "source": [
        "### Setting up Config - to train globally\n",
        "\n",
        "In total there are:\n",
        "- 32 feature stack deep learning experiments\n",
        "- 37 Multiview with 2 input experiments\n",
        "- 8 Multiview with 3 input experiments\n",
        "\n",
        "Each experiment has a different configuration for example, different experiment name, bands for each input layer. Hence, a configuration is neccesary. There is also configuration for training locally in Thailand and is found in Preprocessing_and_export.ipynb notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMRzjEfwNDNi"
      },
      "outputs": [],
      "source": [
        "configs_fs_global = {}\n",
        "train_size = 72*10\n",
        "eval_size = 72*3\n",
        "\n",
        "#### Feature stack experiment\n",
        "\n",
        "configs_fs_global[\"S1A_el_sl_as\"] = config.configuration(\"S1A_el_sl_as\", [\"VV\", \"VH\", \"angle\", \"elevation\", \"slope\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1A_el\"] = config.configuration(\"S1A_el\", [\"VV\", \"VH\", \"angle\", \"elevation\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1A_sl\"] = config.configuration(\"S1A_sl\", [\"VV\", \"VH\", \"angle\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1A_as\"] = config.configuration(\"S1A_as\", [\"VV\", \"VH\", \"angle\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1A_sl_as\"] = config.configuration(\"S1A_sl_as\", [\"VV\", \"VH\", \"angle\", \"slope\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1A_el_sl\"] = config.configuration(\"S1A_el_sl\", [\"VV\", \"VH\", \"angle\", \"elevation\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1A_el_as\"] = config.configuration(\"S1A_el_as\", [\"VV\", \"VH\", \"angle\", \"elevation\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "\n",
        "configs_fs_global[\"S1_el_sl_as\"] = config.configuration(\"S1_el_sl_as\", [\"VV\", \"VH\", \"elevation\", \"slope\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1_el\"] = config.configuration(\"S1_el\", [\"VV\", \"VH\", \"elevation\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1_sl\"] = config.configuration(\"S1_sl\", [\"VV\", \"VH\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1_as\"] = config.configuration(\"S1_as\", [\"VV\", \"VH\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1_sl_as\"] = config.configuration(\"S1_sl_as\", [\"VV\", \"VH\", \"slope\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1_el_sl\"] = config.configuration(\"S1_el_sl\", [\"VV\", \"VH\", \"elevation\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1_el_as\"] = config.configuration(\"S1_el_as\", [\"VV\", \"VH\", \"elevation\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "\n",
        "configs_fs_global[\"L8SR_el_sl_as\"] = config.configuration(\"L8SR_el_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\", \"slope\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_el\"] = config.configuration(\"L8SR_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_sl\"] = config.configuration(\"L8SR_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_as\"] = config.configuration(\"L8SR_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_sl_as\"] = config.configuration(\"L8SR_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"slope\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_el_sl\"] = config.configuration(\"L8SR_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_el_as\"] = config.configuration(\"L8SR_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"elevation\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "\n",
        "configs_fs_global[\"L8SR_S1_el\"] = config.configuration(\"L8SR_S1_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"elevation\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_S1_sl\"] = config.configuration(\"L8SR_S1_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_S1_sl_el_as\"] = config.configuration(\"L8SR_S1_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"slope\",\"elevation\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "\n",
        "configs_fs_global[\"L8SR_S1A_el\"] = config.configuration(\"L8SR_S1A_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\", \"elevation\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_S1A_sl\"] = config.configuration(\"L8SR_S1A_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\", \"slope\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_S1A_sl_el_as\"] = config.configuration(\"L8SR_S1A_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\", \"slope\",\"elevation\", \"aspect\"], train_size, eval_size, country = \"global\")\n",
        "\n",
        "configs_fs_global[\"L8SR\"] = config.configuration(\"L8SR\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1\"] = config.configuration(\"S1\", [\"VV\", \"VH\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"S1A\"] = config.configuration(\"S1A\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_S1\"] = config.configuration(\"L8SR_S1\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\"], train_size, eval_size, country = \"global\")\n",
        "configs_fs_global[\"L8SR_S1A\"] = config.configuration(\"L8SR_S1A\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"VV\", \"VH\", \"angle\"], train_size, eval_size, country = \"global\")\n",
        "\n",
        "###### Multiexperiment\n",
        "\n",
        "configs_multi_global = {}\n",
        "\n",
        "configs_multi_global[\"S1A_el_sl_as\"] = config.configuration(\"S1A_el_sl_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\", \"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1A_el\"] = config.configuration(\"S1A_el\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1A_sl\"] = config.configuration(\"S1A_sl\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1A_as\"] = config.configuration(\"S1A_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1A_sl_as\"] = config.configuration(\"S1A_sl_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1A_el_sl\"] = config.configuration(\"S1A_el_sl\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\", \"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1A_el_as\"] = config.configuration(\"S1A_el_as\", [\"VV\", \"VH\", \"angle\"], train_size, eval_size, [\"elevation\", \"aspect\"], type_=2, country = \"global\")\n",
        "\n",
        "configs_multi_global[\"S1_el_sl_as\"] = config.configuration(\"S1_el_sl_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\", \"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1_el\"] = config.configuration(\"S1_el\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1_sl\"] = config.configuration(\"S1_sl\", [\"VV\", \"VH\"], train_size, eval_size, [\"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1_as\"] = config.configuration(\"S1_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1_sl_as\"] = config.configuration(\"S1_sl_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1_el_sl\"] = config.configuration(\"S1_el_sl\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\", \"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"S1_el_as\"] = config.configuration(\"S1_el_as\", [\"VV\", \"VH\"], train_size, eval_size, [\"elevation\", \"aspect\"], type_=2, country = \"global\")\n",
        "\n",
        "configs_multi_global[\"L8SR_el_sl_as\"] = config.configuration(\"L8SR_el_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\", \"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_el\"] = config.configuration(\"L8SR_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_sl\"] = config.configuration(\"L8SR_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_as\"] = config.configuration(\"L8SR_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_sl_as\"] = config.configuration(\"L8SR_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_el_sl\"] = config.configuration(\"L8SR_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\", \"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_el_as\"] = config.configuration(\"L8SR_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"elevation\", \"aspect\"], type_=2, country = \"global\")\n",
        "\n",
        "\n",
        "configs_multi_global[\"L8SR_S1_as\"] = config.configuration(\"L8SR_S1_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1_el\"] = config.configuration(\"L8SR_S1_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"elevation\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1_sl\"] = config.configuration(\"L8SR_S1_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"slope\"], type_=2, country = \"global\")\n",
        "\n",
        "configs_multi_global[\"L8SR_S1_sl_as\"] = config.configuration(\"L8SR_S1_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1_el_sl\"] = config.configuration(\"L8SR_S1_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"elevation\", \"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1_el_as\"] = config.configuration(\"L8SR_S1_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"elevation\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1_sl_el_as\"] = config.configuration(\"L8SR_S1_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"slope\",\"elevation\", \"aspect\"], type_=2, country = \"global\")\n",
        "\n",
        "configs_multi_global[\"L8SR_S1A_as\"] = config.configuration(\"L8SR_S1A_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1A_el\"] = config.configuration(\"L8SR_S1A_el\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"elevation\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1A_sl\"] = config.configuration(\"L8SR_S1A_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"slope\"], type_=2, country = \"global\")\n",
        "\n",
        "configs_multi_global[\"L8SR_S1A_sl_as\"] = config.configuration(\"L8SR_S1A_sl_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"slope\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1A_el_sl\"] = config.configuration(\"L8SR_S1A_el_sl\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"elevation\", \"slope\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1A_el_as\"] = config.configuration(\"L8SR_S1A_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"elevation\", \"aspect\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1A_sl_el_as\"] = config.configuration(\"L8SR_S1A_sl_el_as\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\", \"slope\",\"elevation\", \"aspect\"], type_=2, country = \"global\")\n",
        "\n",
        "configs_multi_global[\"L8SR_S1\"] = config.configuration(\"L8SR_S1\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], type_=2, country = \"global\")\n",
        "configs_multi_global[\"L8SR_S1A\"] = config.configuration(\"L8SR_S1A\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], type_=2, country = \"global\")\n",
        "\n",
        "configs_multi_3_global = {}\n",
        "\n",
        "configs_multi_3_global[\"L8SR_S1_as3\"] = config.configuration(\"L8SR_S1_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"aspect\"], type_=3, country = \"global\")\n",
        "configs_multi_3_global[\"L8SR_S1_el3\"] = config.configuration(\"L8SR_S1_el3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"elevation\"], type_=3, country = \"global\")\n",
        "configs_multi_3_global[\"L8SR_S1_sl3\"] = config.configuration(\"L8SR_S1_sl3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"slope\"], type_=3, country = \"global\")\n",
        "configs_multi_3_global[\"L8SR_S1_sl_el_as3\"] = config.configuration(\"L8SR_S1_sl_el_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\"], [\"slope\",\"elevation\", \"aspect\"], type_=3, country = \"global\")\n",
        "\n",
        "configs_multi_3_global[\"L8SR_S1A_as3\"] = config.configuration(\"L8SR_S1A_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"aspect\"], type_=3, country = \"global\")\n",
        "configs_multi_3_global[\"L8SR_S1A_el3\"] = config.configuration(\"L8SR_S1A_el3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"elevation\"], type_=3, country = \"global\")\n",
        "configs_multi_3_global[\"L8SR_S1A_sl3\"] = config.configuration(\"L8SR_S1A_sl3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"slope\"], type_=3, country = \"global\")\n",
        "configs_multi_3_global[\"L8SR_S1A_sl_el_as3\"] = config.configuration(\"L8SR_S1A_sl_el_as3\", [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], train_size, eval_size, [\"VV\", \"VH\", \"angle\"], [\"slope\",\"elevation\", \"aspect\"], type_=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Br0sJObNZKZ"
      },
      "outputs": [],
      "source": [
        "# Feature Stack\n",
        "\n",
        "configs_fs[\"S1A_el_sl_as\"].image = ee.Image.cat([S1A, elevation, slope, aspect]).float()\n",
        "configs_fs[\"S1A_el\"].image = ee.Image.cat([S1A, elevation]).float()\n",
        "configs_fs[\"S1A_sl\"].image = ee.Image.cat([S1A, slope]).float()\n",
        "configs_fs[\"S1A_as\"].image = ee.Image.cat([S1A, aspect]).float()\n",
        "configs_fs[\"S1A_sl_as\"].image = ee.Image.cat([S1A, slope, aspect]).float()\n",
        "configs_fs[\"S1A_el_sl\"].image = ee.Image.cat([S1A, elevation, slope]).float()\n",
        "configs_fs[\"S1A_el_as\"].image = ee.Image.cat([S1A, elevation, aspect]).float()\n",
        "\n",
        "configs_fs[\"S1_el_sl_as\"].image = ee.Image.cat([S1, elevation, slope, aspect]).float()\n",
        "configs_fs[\"S1_el\"].image = ee.Image.cat([S1, elevation]).float()\n",
        "configs_fs[\"S1_sl\"].image = ee.Image.cat([S1, slope]).float()\n",
        "configs_fs[\"S1_as\"].image = ee.Image.cat([S1, aspect]).float()\n",
        "configs_fs[\"S1_sl_as\"].image = ee.Image.cat([S1, slope, aspect]).float()\n",
        "configs_fs[\"S1_el_sl\"].image = ee.Image.cat([S1, elevation, slope]).float()\n",
        "configs_fs[\"S1_el_as\"].image = ee.Image.cat([S1, elevation, aspect]).float()\n",
        "\n",
        "configs_fs[\"L8SR_el_sl_as\"].image = ee.Image.cat([L8SR, elevation, slope, aspect]).float()\n",
        "configs_fs[\"L8SR_el\"].image = ee.Image.cat([L8SR, elevation]).float()\n",
        "configs_fs[\"L8SR_sl\"].image = ee.Image.cat([L8SR, slope]).float()\n",
        "configs_fs[\"L8SR_as\"].image = ee.Image.cat([L8SR, aspect]).float()\n",
        "configs_fs[\"L8SR_sl_as\"].image = ee.Image.cat([L8SR, slope, aspect]).float()\n",
        "configs_fs[\"L8SR_el_sl\"].image = ee.Image.cat([L8SR, elevation, slope]).float()\n",
        "configs_fs[\"L8SR_el_as\"].image = ee.Image.cat([L8SR, elevation, aspect]).float()\n",
        "\n",
        "configs_fs[\"L8SR_S1_el\"].image = ee.Image.cat([L8SR, S1, elevation]).float()\n",
        "configs_fs[\"L8SR_S1_sl\"].image = ee.Image.cat([L8SR, S1, slope]).float()\n",
        "configs_fs[\"L8SR_S1_sl_el_as\"].image = ee.Image.cat([L8SR, S1, slope, elevation, aspect]).float()\n",
        "\n",
        "configs_fs[\"L8SR_S1A_el\"].image = ee.Image.cat([L8SR, S1A, elevation]).float()\n",
        "configs_fs[\"L8SR_S1A_sl\"].image = ee.Image.cat([L8SR, S1A, slope]).float()\n",
        "configs_fs[\"L8SR_S1A_sl_el_as\"].image = ee.Image.cat([L8SR, S1A, slope, elevation, aspect]).float()\n",
        "\n",
        "configs_fs[\"L8SR\"].image = L8SR.float()\n",
        "configs_fs[\"S1\"].image = S1.float()\n",
        "configs_fs[\"S1A\"].image = S1A.float()\n",
        "configs_fs[\"L8SR_S1\"].image = ee.Image.cat([L8SR, S1]).float()\n",
        "configs_fs[\"L8SR_S1A\"].image = ee.Image.cat([L8SR, S1A]).float()\n",
        "\n",
        "# Multiview\n",
        "\n",
        "configs_multi[\"S1A_el_sl_as\"].image = ee.Image.cat([S1A, elevation, slope, aspect]).float() \n",
        "configs_multi[\"S1A_el\"].image = ee.Image.cat([S1A, elevation]).float() \n",
        "configs_multi[\"S1A_sl\"].image = ee.Image.cat([S1A, slope]).float() \n",
        "configs_multi[\"S1A_as\"].image = ee.Image.cat([S1A, aspect]).float() \n",
        "configs_multi[\"S1A_sl_as\"].image = ee.Image.cat([S1A, slope, aspect]).float() \n",
        "configs_multi[\"S1A_el_sl\"].image = ee.Image.cat([S1A, elevation, slope]).float() \n",
        "configs_multi[\"S1A_el_as\"].image = ee.Image.cat([S1A, elevation, aspect]).float() \n",
        "\n",
        "configs_multi[\"S1_el_sl_as\"].image = ee.Image.cat([S1, elevation, slope, aspect]).float()  \n",
        "configs_multi[\"S1_el\"].image = ee.Image.cat([S1, elevation]).float()\n",
        "configs_multi[\"S1_sl\"].image = ee.Image.cat([S1, slope]).float() \n",
        "configs_multi[\"S1_as\"].image = ee.Image.cat([S1, aspect]).float() \n",
        "configs_multi[\"S1_sl_as\"].image = ee.Image.cat([S1, slope, aspect]).float()\n",
        "configs_multi[\"S1_el_sl\"].image = ee.Image.cat([S1, elevation, slope]).float() \n",
        "configs_multi[\"S1_el_as\"].image = ee.Image.cat([S1, elevation, aspect]).float() \n",
        "\n",
        "configs_multi[\"L8SR_el_sl_as\"].image = ee.Image.cat([L8SR, elevation, slope, aspect]).float() \n",
        "configs_multi[\"L8SR_el\"].image = ee.Image.cat([L8SR, elevation]).float() \n",
        "configs_multi[\"L8SR_sl\"].image = ee.Image.cat([L8SR, slope]).float() \n",
        "configs_multi[\"L8SR_as\"].image = ee.Image.cat([L8SR, aspect]).float() \n",
        "configs_multi[\"L8SR_sl_as\"].image = ee.Image.cat([L8SR, slope, aspect]).float() \n",
        "configs_multi[\"L8SR_el_sl\"].image = ee.Image.cat([ L8SR, elevation, slope]).float() \n",
        "configs_multi[\"L8SR_el_as\"].image = ee.Image.cat([L8SR, elevation, aspect]).float() \n",
        "\n",
        "\n",
        "configs_multi[\"L8SR_S1_as\"].image = ee.Image.cat([L8SR, S1, aspect]).float()\n",
        "configs_multi[\"L8SR_S1_el\"].image = ee.Image.cat([L8SR, S1, elevation]).float() \n",
        "configs_multi[\"L8SR_S1_sl\"].image = ee.Image.cat([L8SR, S1, slope]).float() \n",
        "\n",
        "configs_multi[\"L8SR_S1_sl_as\"].image = ee.Image.cat([L8SR, S1, slope, aspect]).float() \n",
        "configs_multi[\"L8SR_S1_el_sl\"].image = ee.Image.cat([L8SR, S1, elevation, slope]).float() \n",
        "configs_multi[\"L8SR_S1_el_as\"].image = ee.Image.cat([L8SR, slope, elevation, aspect]).float() \n",
        "configs_multi[\"L8SR_S1_sl_el_as\"].image = ee.Image.cat([L8SR, S1, slope, elevation, aspect]).float() \n",
        "\n",
        "configs_multi[\"L8SR_S1A_as\"].image = ee.Image.cat([L8SR, S1A, aspect]).float()\n",
        "configs_multi[\"L8SR_S1A_el\"].image = ee.Image.cat([L8SR, S1A, elevation]).float() \n",
        "configs_multi[\"L8SR_S1A_sl\"].image = ee.Image.cat([L8SR, S1A, slope]).float() \n",
        "\n",
        "configs_multi[\"L8SR_S1A_sl_as\"].image = ee.Image.cat([L8SR, S1A, slope, aspect]).float() \n",
        "configs_multi[\"L8SR_S1A_el_sl\"].image = ee.Image.cat([L8SR, S1A, elevation, slope]).float() \n",
        "configs_multi[\"L8SR_S1A_el_as\"].image = ee.Image.cat([L8SR, S1A, elevation, aspect]).float() \n",
        "configs_multi[\"L8SR_S1A_sl_el_as\"].image = ee.Image.cat([L8SR, S1A, slope, elevation, aspect]).float() \n",
        "\n",
        "configs_multi[\"L8SR_S1\"].image = ee.Image.cat([L8SR, S1]).float() \n",
        "configs_multi[\"L8SR_S1A\"].image = ee.Image.cat([L8SR, S1A]).float() \n",
        "\n",
        "# Multiview-3\n",
        "\n",
        "\n",
        "configs_multi_3[\"L8SR_S1_as3\"].image = ee.Image.cat([L8SR, S1, aspect]).float()\n",
        "configs_multi_3[\"L8SR_S1_el3\"].image = ee.Image.cat([L8SR, S1, elevation]).float()\n",
        "configs_multi_3[\"L8SR_S1_sl3\"].image = ee.Image.cat([L8SR, S1, slope]).float()\n",
        "configs_multi_3[\"L8SR_S1_sl_el_as3\"].image = ee.Image.cat([L8SR, S1, slope, elevation, aspect]).float()\n",
        "\n",
        "configs_multi_3[\"L8SR_S1A_as3\"].image = ee.Image.cat([L8SR, S1A, aspect]).float()\n",
        "configs_multi_3[\"L8SR_S1A_el3\"].image = ee.Image.cat([L8SR, S1A, elevation]).float()\n",
        "configs_multi_3[\"L8SR_S1A_sl3\"].image = ee.Image.cat([L8SR, S1A, slope]).float()\n",
        "configs_multi_3[\"L8SR_S1A_sl_el_as3\"].image = ee.Image.cat([L8SR, S1A, slope, elevation, aspect]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_86_F_6PNb55"
      },
      "outputs": [],
      "source": [
        "# Feature Stack\n",
        "\n",
        "configs_fs_global[\"S1A_el_sl_as\"].image = ee.Image.cat([S1A, elevation, slope, aspect]).float()\n",
        "configs_fs_global[\"S1A_el\"].image = ee.Image.cat([S1A, elevation]).float()\n",
        "configs_fs_global[\"S1A_sl\"].image = ee.Image.cat([S1A, slope]).float()\n",
        "configs_fs_global[\"S1A_as\"].image = ee.Image.cat([S1A, aspect]).float()\n",
        "configs_fs_global[\"S1A_sl_as\"].image = ee.Image.cat([S1A, slope, aspect]).float()\n",
        "configs_fs_global[\"S1A_el_sl\"].image = ee.Image.cat([S1A, elevation, slope]).float()\n",
        "configs_fs_global[\"S1A_el_as\"].image = ee.Image.cat([S1A, elevation, aspect]).float()\n",
        "\n",
        "configs_fs_global[\"S1_el_sl_as\"].image = ee.Image.cat([S1, elevation, slope, aspect]).float()\n",
        "configs_fs_global[\"S1_el\"].image = ee.Image.cat([S1, elevation]).float()\n",
        "configs_fs_global[\"S1_sl\"].image = ee.Image.cat([S1, slope]).float()\n",
        "configs_fs_global[\"S1_as\"].image = ee.Image.cat([S1, aspect]).float()\n",
        "configs_fs_global[\"S1_sl_as\"].image = ee.Image.cat([S1, slope, aspect]).float()\n",
        "configs_fs_global[\"S1_el_sl\"].image = ee.Image.cat([S1, elevation, slope]).float()\n",
        "configs_fs_global[\"S1_el_as\"].image = ee.Image.cat([S1, elevation, aspect]).float()\n",
        "\n",
        "configs_fs_global[\"L8SR_el_sl_as\"].image = ee.Image.cat([L8SR, elevation, slope, aspect]).float()\n",
        "configs_fs_global[\"L8SR_el\"].image = ee.Image.cat([L8SR, elevation]).float()\n",
        "configs_fs_global[\"L8SR_sl\"].image = ee.Image.cat([L8SR, slope]).float()\n",
        "configs_fs_global[\"L8SR_as\"].image = ee.Image.cat([L8SR, aspect]).float()\n",
        "configs_fs_global[\"L8SR_sl_as\"].image = ee.Image.cat([L8SR, slope, aspect]).float()\n",
        "configs_fs_global[\"L8SR_el_sl\"].image = ee.Image.cat([L8SR, elevation, slope]).float()\n",
        "configs_fs_global[\"L8SR_el_as\"].image = ee.Image.cat([L8SR, elevation, aspect]).float()\n",
        "\n",
        "configs_fs_global[\"L8SR_S1_el\"].image = ee.Image.cat([L8SR, S1, elevation]).float()\n",
        "configs_fs_global[\"L8SR_S1_sl\"].image = ee.Image.cat([L8SR, S1, slope]).float()\n",
        "configs_fs_global[\"L8SR_S1_sl_el_as\"].image = ee.Image.cat([L8SR, S1, slope, elevation, aspect]).float()\n",
        "\n",
        "configs_fs_global[\"L8SR_S1A_el\"].image = ee.Image.cat([L8SR, S1A, elevation]).float()\n",
        "configs_fs_global[\"L8SR_S1A_sl\"].image = ee.Image.cat([L8SR, S1A, slope]).float()\n",
        "configs_fs_global[\"L8SR_S1A_sl_el_as\"].image = ee.Image.cat([L8SR, S1A, slope, elevation, aspect]).float()\n",
        "\n",
        "configs_fs_global[\"L8SR\"].image = L8SR.float()\n",
        "configs_fs_global[\"S1\"].image = S1.float()\n",
        "configs_fs_global[\"S1A\"].image = S1A.float()\n",
        "configs_fs_global[\"L8SR_S1\"].image = ee.Image.cat([L8SR, S1]).float()\n",
        "configs_fs_global[\"L8SR_S1A\"].image = ee.Image.cat([L8SR, S1A]).float()\n",
        "\n",
        "\n",
        "# Multiview\n",
        "\n",
        "configs_multi_global[\"S1A_el_sl_as\"].image = ee.Image.cat([S1A, elevation, slope, aspect]).float() \n",
        "configs_multi_global[\"S1A_el\"].image = ee.Image.cat([S1A, elevation]).float() \n",
        "configs_multi_global[\"S1A_sl\"].image = ee.Image.cat([S1A, slope]).float() \n",
        "configs_multi_global[\"S1A_as\"].image = ee.Image.cat([S1A, aspect]).float() \n",
        "configs_multi_global[\"S1A_sl_as\"].image = ee.Image.cat([S1A, slope, aspect]).float() \n",
        "configs_multi_global[\"S1A_el_sl\"].image = ee.Image.cat([S1A, elevation, slope]).float() \n",
        "configs_multi_global[\"S1A_el_as\"].image = ee.Image.cat([S1A, elevation, aspect]).float() \n",
        "\n",
        "configs_multi_global[\"S1_el_sl_as\"].image = ee.Image.cat([S1, elevation, slope, aspect]).float()  \n",
        "configs_multi_global[\"S1_el\"].image = ee.Image.cat([S1, elevation]).float()\n",
        "configs_multi_global[\"S1_sl\"].image = ee.Image.cat([S1, slope]).float() \n",
        "configs_multi_global[\"S1_as\"].image = ee.Image.cat([S1, aspect]).float() \n",
        "configs_multi_global[\"S1_sl_as\"].image = ee.Image.cat([S1, slope, aspect]).float()\n",
        "configs_multi_global[\"S1_el_sl\"].image = ee.Image.cat([S1, elevation, slope]).float() \n",
        "configs_multi_global[\"S1_el_as\"].image = ee.Image.cat([S1, elevation, aspect]).float() \n",
        "\n",
        "configs_multi_global[\"L8SR_el_sl_as\"].image = ee.Image.cat([L8SR, elevation, slope, aspect]).float() \n",
        "configs_multi_global[\"L8SR_el\"].image = ee.Image.cat([L8SR, elevation]).float() \n",
        "configs_multi_global[\"L8SR_sl\"].image = ee.Image.cat([L8SR, slope]).float() \n",
        "configs_multi_global[\"L8SR_as\"].image = ee.Image.cat([L8SR, aspect]).float() \n",
        "configs_multi_global[\"L8SR_sl_as\"].image = ee.Image.cat([L8SR, slope, aspect]).float() \n",
        "configs_multi_global[\"L8SR_el_sl\"].image = ee.Image.cat([ L8SR, elevation, slope]).float() \n",
        "configs_multi_global[\"L8SR_el_as\"].image = ee.Image.cat([L8SR, elevation, aspect]).float() \n",
        "\n",
        "\n",
        "configs_multi_global[\"L8SR_S1_as\"].image = ee.Image.cat([L8SR, S1, aspect]).float()\n",
        "configs_multi_global[\"L8SR_S1_el\"].image = ee.Image.cat([L8SR, S1, elevation]).float() \n",
        "configs_multi_global[\"L8SR_S1_sl\"].image = ee.Image.cat([L8SR, S1, slope]).float() \n",
        "\n",
        "configs_multi_global[\"L8SR_S1_sl_as\"].image = ee.Image.cat([L8SR, S1, slope, aspect]).float() \n",
        "configs_multi_global[\"L8SR_S1_el_sl\"].image = ee.Image.cat([L8SR, S1, elevation, slope]).float() \n",
        "configs_multi_global[\"L8SR_S1_el_as\"].image = ee.Image.cat([L8SR, slope, elevation, aspect]).float() \n",
        "configs_multi_global[\"L8SR_S1_sl_el_as\"].image = ee.Image.cat([L8SR, S1, slope, elevation, aspect]).float() \n",
        "\n",
        "configs_multi_global[\"L8SR_S1A_as\"].image = ee.Image.cat([L8SR, S1A, aspect]).float()\n",
        "configs_multi_global[\"L8SR_S1A_el\"].image = ee.Image.cat([L8SR, S1A, elevation]).float() \n",
        "configs_multi_global[\"L8SR_S1A_sl\"].image = ee.Image.cat([L8SR, S1A, slope]).float() \n",
        "\n",
        "configs_multi_global[\"L8SR_S1A_sl_as\"].image = ee.Image.cat([L8SR, S1A, slope, aspect]).float() \n",
        "configs_multi_global[\"L8SR_S1A_el_sl\"].image = ee.Image.cat([L8SR, S1A, elevation, slope]).float() \n",
        "configs_multi_global[\"L8SR_S1A_el_as\"].image = ee.Image.cat([L8SR, S1A, elevation, aspect]).float() \n",
        "configs_multi_global[\"L8SR_S1A_sl_el_as\"].image = ee.Image.cat([L8SR, S1A, slope, elevation, aspect]).float() \n",
        "\n",
        "configs_multi_global[\"L8SR_S1\"].image = ee.Image.cat([L8SR, S1]).float() \n",
        "configs_multi_global[\"L8SR_S1A\"].image = ee.Image.cat([L8SR, S1A]).float() \n",
        "\n",
        "# Multiview-3\n",
        "\n",
        "\n",
        "configs_multi_3_global[\"L8SR_S1_as3\"].image = ee.Image.cat([L8SR, S1, aspect])\n",
        "configs_multi_3_global[\"L8SR_S1_el3\"].image = ee.Image.cat([L8SR, S1, elevation])\n",
        "configs_multi_3_global[\"L8SR_S1_sl3\"].image = ee.Image.cat([L8SR, S1, slope])\n",
        "configs_multi_3_global[\"L8SR_S1_sl_el_as3\"].image = ee.Image.cat([L8SR, S1, slope, elevation, aspect])\n",
        "\n",
        "configs_multi_3_global[\"L8SR_S1A_as3\"].image = ee.Image.cat([L8SR, S1A, aspect])\n",
        "configs_multi_3_global[\"L8SR_S1A_el3\"].image = ee.Image.cat([L8SR, S1A, elevation])\n",
        "configs_multi_3_global[\"L8SR_S1A_sl3\"].image = ee.Image.cat([L8SR, S1A, slope])\n",
        "configs_multi_3_global[\"L8SR_S1A_sl_el_as3\"].image = ee.Image.cat([L8SR, S1A, slope, elevation, aspect])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BBcD2qhNinz"
      },
      "source": [
        "### Export Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYf1Fg3aNkMg"
      },
      "source": [
        "\n",
        "\n",
        "Initially we load the model of interest, and configure where we export the image of interest. For demonstration purposes we selected 10 regions in\n",
        "- Thailand\n",
        "- Tibet\n",
        "- Ghana\n",
        "- brazil\n",
        "- mexico\n",
        "- pakistan\n",
        "- egypt\n",
        "- cambodia\n",
        "- India"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUkI2zARNi2z"
      },
      "outputs": [],
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/mewchayutaphong' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# Half this will extend on the sides of each patch.\n",
        "kernel_buffer = [128, 128]\n",
        "\n",
        "th_image_base = f'Thailand_with_Multi_'\n",
        "th_region = ee.Geometry.BBox(100.30632852425321, 17.709225431372587, 100.74128946175321, 18.20417872756825)\n",
        "\n",
        "tb_image_base = f'Tibet_with_Multi_'\n",
        "tb_region = ee.Geometry.BBox(83.7866460908476, 31.02991423438545, 84.4782964814726, 31.623526673040716)\n",
        "\n",
        "gm_image_base = f'Ghana_with_Multi_'\n",
        "gm_region = ee.Geometry.BBox(-1.9983132238272127, 5.925449378444892, -1.8517086095694002, 6.081004042776062)\n",
        "\n",
        "brazil_image_base = f'brazil_with_Multi_'\n",
        "brazil_region = ee.Geometry.BBox(-56.914136208542024, -27.726731964448247, -55.734106911667024, -27.158518840235534)\n",
        "\n",
        "mexico_image_base = f'mexico_with_Multi_'\n",
        "mexico_region = ee.Geometry.BBox(-93.99463576763688, 16.864181463158733, -93.24756545513688, 17.352438403346977)\n",
        "\n",
        "pakistan_image_base = f'pakistan_with_Multi_'\n",
        "pakistan_region = ee.Geometry.BBox(70.62737898109093, 29.031658472574723, 70.86907819984093, 29.247564938104578)\n",
        "\n",
        "egypt_image_base = f'egypt_with_Multi_'\n",
        "egypt_region = ee.Geometry.BBox(32.25680416139795, 22.776830239482934, 33.28951900514795, 23.267354217706654)\n",
        "\n",
        "cambodia_image_base = f'cambodia_with_Multi_'\n",
        "cambodia_region = ee.Geometry.BBox(103.7086527335668, 12.395622357749016, 104.5710794913793, 13.2151653909455)\n",
        "\n",
        "India_image_base = f'India_with_Multi_'\n",
        "India_region = ee.Geometry.BBox(80.89907945596022, 23.91896359428029, 81.32754625283522, 24.24995005933742)\n",
        "\n",
        "Bangladesh_image_base = f'Bangladesh_with_Multi_'\n",
        "Bangladesh_region = ee.Geometry.BBox(89.25478625914253, 24.205326578074743, 90.15566516539253, 24.765228419516816)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From model trained in thailand"
      ],
      "metadata": {
        "id": "lmtEGWQIk02c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Here we select the model of interest\n",
        "conf = configs_multi[\"L8SR_S1\"]\n",
        "preproc = preprocessing.Preprocessor(conf)\n",
        "MODEL_DIR = 'gs://' + conf.BUCKET + \"/\" + conf.FOLDER + \"/Models/\" + conf.PROJECT_TITLE + \"_EPOCHS_\" + str(10)\n",
        "\n",
        "## Model is loaded\n",
        "model_custom = tf.keras.models.load_model(MODEL_DIR, custom_objects={'f1':metrics_.f1, \"custom_accuracy\": metrics_.custom_accuracy})\n",
        "\n",
        "## Export the image to GCB before prediction\n",
        "images.doExport(th_image_base, kernel_buffer, th_region, conf)\n",
        "\n",
        "## load the image from GCB and predict the water bodies\n",
        "## Below gives a choice of the type of model interested \n",
        "## Please use doPrediction_featurestack for all FS U-Net model, doPrediction_multiview_2 for M2 U-Net\n",
        "## doPrediction_multiview_3 for M3 U-Net\n",
        "\n",
        "# out_image_asset, out_image_file, jsonFile = doPrediction_featurestack(th_image_base, user_folder, kernel_buffer, model_custom, \"_epochs_10_\", conf)\n",
        "out_image_asset, out_image_file, jsonFile = images.doPrediction_multiview_2(th_image_base, user_folder, kernel_buffer, model_custom, \"multiview-2_epochs_10_\", conf)\n",
        "# out_image_asset, out_image_file, jsonFile = doPrediction_multiview_3(th_image_base, user_folder, kernel_buffer, model_custom, \"_epochs_10_\", conf)\n",
        "\n",
        "## Upload image to GEE\n",
        "!earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "90b185d3e825481ea8cc84e57f319311",
            "afa131ca49b8443db40c9565c1af84f9",
            "1b05f18f1c2f47d2a80f58c5cccf1810",
            "c1053db7dd0a4790b376dd325f46fcf1",
            "1df70e234c1a4a019f1c9e3e25b914f1",
            "65dcfad174d24a7b9113bed5d8acb76c",
            "c0aea30ee1c74e14ba1c906972ab868b",
            "77f49fb692664cf58219fedf1403c6d6",
            "2cd316bcd0b44a1c9eab350875724ad9",
            "a92aae7c5d56463e8e81c10c27716359",
            "78825933fce54de3ab323fef574d9c69"
          ]
        },
        "id": "6gq2QpVci89m",
        "outputId": "1d84a2ac-f7c5-4041-dbce-4df8d0c6cd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_2899690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running image export to Cloud Storage...\n",
            "Image export completed.\n",
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/m2_TH_Cnn_L8SR_S1/', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Ghana_with_MultiL8SR_S1.TFRecord', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Ghana_with_MultiL8SR_S1.json', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Ghana_with_MultiL8SR_S1.tfrecord.gz', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_00000.tfrecord.gz', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_00001.tfrecord.gz', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_mixer.json', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Models/', '']\n",
            "['gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_00000.tfrecord.gz', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_00001.tfrecord.gz', 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_mixer.json']\n",
            "['gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_00000.tfrecord.gz',\n",
            " 'gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_00001.tfrecord.gz']\n",
            "gs://geebucketwater/m2_TH_Cnn_L8SR_S1/Thailand_with_Multi_mixer.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 6,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            100.30615411937102,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            18.204359232682105]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 42}\n",
            "Running predictions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/42 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90b185d3e825481ea8cc84e57f319311"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Writing patch 12...\n",
            "Writing patch 13...\n",
            "Writing patch 14...\n",
            "Writing patch 15...\n",
            "Writing patch 16...\n",
            "Writing patch 17...\n",
            "Writing patch 18...\n",
            "Writing patch 19...\n",
            "Writing patch 20...\n",
            "Writing patch 21...\n",
            "Writing patch 22...\n",
            "Writing patch 23...\n",
            "Writing patch 24...\n",
            "Writing patch 25...\n",
            "Writing patch 26...\n",
            "Writing patch 27...\n",
            "Writing patch 28...\n",
            "Writing patch 29...\n",
            "Writing patch 30...\n",
            "Writing patch 31...\n",
            "Writing patch 32...\n",
            "Writing patch 33...\n",
            "Writing patch 34...\n",
            "Writing patch 35...\n",
            "Writing patch 36...\n",
            "Writing patch 37...\n",
            "Writing patch 38...\n",
            "Writing patch 39...\n",
            "Writing patch 40...\n",
            "Writing patch 41...\n",
            "Started upload task with ID: J6HDIKD6WIIASO6EWFJPTH4Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9MluYTCNtxU"
      },
      "source": [
        "#### From model trained globally"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Here we select the model of interest from the global configuration\n",
        "conf = configs_multi_global[\"L8SR_S1\"]\n",
        "preproc = preprocessing.Preprocessor(conf)\n",
        "MODEL_DIR = 'gs://' + conf.BUCKET + \"/\" + conf.FOLDER + \"/Models/\" + conf.PROJECT_TITLE + \"_EPOCHS_\" + str(10)\n",
        "\n",
        "## Model is loaded\n",
        "model_custom = tf.keras.models.load_model(MODEL_DIR, custom_objects={'f1':metrics_.f1, \"custom_accuracy\": metrics_.custom_accuracy})\n",
        "\n",
        "## Export the image to GCB before prediction\n",
        "images.doExport(th_image_base, kernel_buffer, th_region, conf)\n",
        "\n",
        "## load the image from GCB and predict the water bodies\n",
        "## Below gives a choice of the type of model interested \n",
        "## Please use doPrediction_featurestack for all FS U-Net model, doPrediction_multiview_2 for M2 U-Net\n",
        "## doPrediction_multiview_3 for M3 U-Net\n",
        "\n",
        "# out_image_asset, out_image_file, jsonFile = doPrediction_featurestack(th_image_base, user_folder, kernel_buffer, model_custom, \"_epochs_10_\", conf)\n",
        "out_image_asset, out_image_file, jsonFile = images.doPrediction_multiview_2(th_image_base, user_folder, kernel_buffer, model_custom, \"multiview-2_epochs_10_\", conf)\n",
        "# out_image_asset, out_image_file, jsonFile = doPrediction_multiview_3(th_image_base, user_folder, kernel_buffer, model_custom, \"_epochs_10_\", conf)\n",
        "\n",
        "## Upload image to GEE\n",
        "!earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n"
      ],
      "metadata": {
        "id": "m-B4CU8lkljy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYMnMX1veFWd"
      },
      "source": [
        "## Objective 2: Dem Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, the S1, L8SR, S1_sl, L8SR + (S1_sl) and L8SR + S1A + sl model will be compared against each other through predicting the image at Tibet during Jan-Feb 2018.\n",
        "\n",
        "This will enable us to visualize the effect of adding feature from the digital elevation model on landsat-8 and sentinel-1. In particular, slope will improve the prediction of S1 and if L8SR is paired through multiple views the performance would also improve"
      ],
      "metadata": {
        "id": "nOGXU_I-pm_Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmFBSkGTeETg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6e2bbfe-05d9-4749-963a-67dee957cd46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_3861075) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running image export to Cloud Storage...\n",
            "Image export completed.\n",
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/fs_global_Cnn_S1/', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_.TFRecord', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_.json', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_.tfrecord.gz', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.TFRecord', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.json', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.tfrecord.gz', 'gs://geebucketwater/fs_global_Cnn_S1/Models/', '']\n",
            "['gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.TFRecord', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.json', 'gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.tfrecord.gz']\n",
            "['gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.tfrecord.gz']\n",
            "gs://geebucketwater/fs_global_Cnn_S1/Tibet_with_Multi_S1.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 10,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            83.78640553899825,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            31.624111599086813]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 80}\n",
            "Running predictions...\n",
            "80/80 [==============================] - 115s 1s/step\n",
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Writing patch 12...\n",
            "Writing patch 13...\n",
            "Writing patch 14...\n",
            "Writing patch 15...\n",
            "Writing patch 16...\n",
            "Writing patch 17...\n",
            "Writing patch 18...\n",
            "Writing patch 19...\n",
            "Writing patch 20...\n",
            "Writing patch 21...\n",
            "Writing patch 22...\n",
            "Writing patch 23...\n",
            "Writing patch 24...\n",
            "Writing patch 25...\n",
            "Writing patch 26...\n",
            "Writing patch 27...\n",
            "Writing patch 28...\n",
            "Writing patch 29...\n",
            "Writing patch 30...\n",
            "Writing patch 31...\n",
            "Writing patch 32...\n",
            "Writing patch 33...\n",
            "Writing patch 34...\n",
            "Writing patch 35...\n",
            "Writing patch 36...\n",
            "Writing patch 37...\n",
            "Writing patch 38...\n",
            "Writing patch 39...\n",
            "Writing patch 40...\n",
            "Writing patch 41...\n",
            "Writing patch 42...\n",
            "Writing patch 43...\n",
            "Writing patch 44...\n",
            "Writing patch 45...\n",
            "Writing patch 46...\n",
            "Writing patch 47...\n",
            "Writing patch 48...\n",
            "Writing patch 49...\n",
            "Writing patch 50...\n",
            "Writing patch 51...\n",
            "Writing patch 52...\n",
            "Writing patch 53...\n",
            "Writing patch 54...\n",
            "Writing patch 55...\n",
            "Writing patch 56...\n",
            "Writing patch 57...\n",
            "Writing patch 58...\n",
            "Writing patch 59...\n",
            "Writing patch 60...\n",
            "Writing patch 61...\n",
            "Writing patch 62...\n",
            "Writing patch 63...\n",
            "Writing patch 64...\n",
            "Writing patch 65...\n",
            "Writing patch 66...\n",
            "Writing patch 67...\n",
            "Writing patch 68...\n",
            "Writing patch 69...\n",
            "Writing patch 70...\n",
            "Writing patch 71...\n",
            "Writing patch 72...\n",
            "Writing patch 73...\n",
            "Writing patch 74...\n",
            "Writing patch 75...\n",
            "Writing patch 76...\n",
            "Writing patch 77...\n",
            "Writing patch 78...\n",
            "Writing patch 79...\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_3823445) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4daf9c58eeca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mpreproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mMODEL_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gs://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUCKET\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROJECT_TITLE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_EPOCHS_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mmodel_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmetrics_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mtb_image_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Tibet_with_Multi_{conf.PROJECT_TITLE}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoExport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb_image_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_region\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   loaded = tf.__internal__.saved_model.load_partial(\n\u001b[0;32m--> 142\u001b[0;31m       path, nodes_to_load, options=options)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;31m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mnode\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mto\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m   \"\"\"\n\u001b[0;32m--> 842\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 975\u001b[0;31m                             ckpt_options, options, filters)\n\u001b[0m\u001b[1;32m    976\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_skip_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturableResource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         options=options)\n\u001b[1;32m   1389\u001b[0m     base.CheckpointPosition(\n\u001b[0;32m-> 1390\u001b[0;31m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[0;31m# Attached dependencies are not attached to the root, so should be restored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         current_position.checkpoint.restore_saveables(tensor_saveables,\n\u001b[1;32m   1100\u001b[0m                                                       \u001b[0mpython_saveables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                                                       registered_savers))\n\u001b[0m\u001b[1;32m   1102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_saveables, registered_savers)\u001b[0m\n\u001b[1;32m    343\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[1;32m    344\u001b[0m           \u001b[0mvalidated_saveables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           registered_savers).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[1;32m    346\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    409\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    392\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_savers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       restored_tensors = io_ops.restore_v2(\n\u001b[0;32m--> 106\u001b[0;31m           file_prefix, tensor_names, tensor_slices, tensor_dtypes)\n\u001b[0m\u001b[1;32m    107\u001b[0m     structured_restored_tensors = nest.pack_sequence_as(\n\u001b[1;32m    108\u001b[0m         tensor_structure, restored_tensors)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mrestore_v2\u001b[0;34m(prefix, tensor_names, shape_and_slices, dtypes, name)\u001b[0m\n\u001b[1;32m   1490\u001b[0m       return restore_v2_eager_fallback(\n\u001b[1;32m   1491\u001b[0m           \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_and_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m           ctx=_ctx)\n\u001b[0m\u001b[1;32m   1493\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mrestore_v2_eager_fallback\u001b[0;34m(prefix, tensor_names, shape_and_slices, dtypes, name, ctx)\u001b[0m\n\u001b[1;32m   1528\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m   _result = _execute.execute(b\"RestoreV2\", len(dtypes), inputs=_inputs_flat,\n\u001b[0;32m-> 1530\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     _execute.record_gradient(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# DEM Experiment in Tibet\n",
        "# S1 (0.61)\n",
        "# L8SR (0.94)\n",
        "# S1 - sl (0.96) \n",
        "# L8SR+(S1-sl) (0.97) \n",
        "# L8SR+S1A+sl (0.96)\n",
        "\n",
        "tb_region = ee.Geometry.BBox(83.7866460908476, 31.02991423438545, 84.4782964814726, 31.623526673040716)\n",
        "\n",
        "DEM_EXP = [configs_fs_global[\"S1\"], configs_fs_global[\"L8SR\"], \\\n",
        "           configs_fs_global[\"S1_sl\"], configs_multi_global[\"L8SR_S1_sl\"], configs_multi_3_global[\"L8SR_S1A_sl3\"]]\n",
        "\n",
        "TRAIN_SIZE = 72*10\n",
        "EVAL_SIZE = 72*3\n",
        "\n",
        "for i, conf in enumerate(DEM_EXP):\n",
        "  print(i)\n",
        "  preproc = preprocessing.Preprocessor(conf)\n",
        "  MODEL_DIR = 'gs://' + conf.BUCKET + \"/\" + conf.FOLDER + \"/Models/\" + conf.PROJECT_TITLE + \"_EPOCHS_\" + str(10)\n",
        "  model_custom = tf.keras.models.load_model(MODEL_DIR, custom_objects={'f1':metrics_.f1})\n",
        "  tb_image_base = f'Tibet_with_Multi_{conf.PROJECT_TITLE}'\n",
        "  images.doExport(tb_image_base, kernel_buffer, tb_region, conf)\n",
        "  if conf.type_ == \"fs\":\n",
        "    out_image_asset, out_image_file, jsonFile = images.doPrediction_featurestack(tb_image_base, user_folder, \\\n",
        "                              kernel_buffer, model_custom, \"DEM_EXP\", conf)\n",
        "  if conf.type_ == \"m2\":\n",
        "    out_image_asset, out_image_file, jsonFile = images.doPrediction_multiview_2(tb_image_base, user_folder, \\\n",
        "                            kernel_buffer, model_custom, \"DEM_EXP\", conf)\n",
        "  if conf.type_ == \"m3\":\n",
        "    out_image_asset, out_image_file, jsonFile = images.doPrediction_multiview_3(tb_image_base, user_folder, \\\n",
        "                            kernel_buffer, model_custom, \"DEM_EXP\", conf)\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFzDqPf_eJNg"
      },
      "source": [
        "## Objective 3: Seasonal Effect experiment - GlobalWaterNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the GlobalWaterNet is used to predict in the same region as in objective 2, but in this case the model will predict seasonally from jan-april, april-july, july-oct, oct-dec. \n",
        "\n",
        "This will enable user to gain insight into the water bodies through time. In addition, it will show that the points which has no-data isn't predicted incorrectly (this can be seen in the paper)."
      ],
      "metadata": {
        "id": "8lIM4RtXqDkz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "de296b78acf141e69d3c49c56c91a1dd",
            "d9fba35ec9a9465b99dd342f00dbc678",
            "6b94a353054b40b6a3708394906e4c84",
            "38d007102f5743d4846a443fb7b9959d",
            "5bb788bb08114a90bfc6b9f2c83ddf6c",
            "44a9ca7c913144de8f5d76ce07e4e457",
            "4a85beda500e4902a8a10433d3d47582",
            "c11996d2ff364e34901acbf581bc5e04",
            "d4b3ccd236e940cb92ca279463f6dbc2",
            "aaad54d3075a499893c9b2eebb717179",
            "64eef717599b4b3f957fb4de68f4e034"
          ]
        },
        "id": "sBU6KTtQjiJD",
        "outputId": "1886e8f9-441f-4452-d73a-65eff91e8def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running image export to Cloud Storage...\n",
            "Image export completed.\n",
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/m2_Global_Cnn_L8SR_S1A_sl_CCDICE_dp0.3/season_0/Tibet_with_Multi_seasonalEXP_00000.tfrecord.gz',\n",
            " 'gs://geebucketwater/m2_Global_Cnn_L8SR_S1A_sl_CCDICE_dp0.3/season_0/Tibet_with_Multi_seasonalEXP_00001.tfrecord.gz',\n",
            " 'gs://geebucketwater/m2_Global_Cnn_L8SR_S1A_sl_CCDICE_dp0.3/season_0/Tibet_with_Multi_seasonalEXP_00002.tfrecord.gz',\n",
            " 'gs://geebucketwater/m2_Global_Cnn_L8SR_S1A_sl_CCDICE_dp0.3/season_0/Tibet_with_Multi_seasonalEXP_00003.tfrecord.gz',\n",
            " 'gs://geebucketwater/m2_Global_Cnn_L8SR_S1A_sl_CCDICE_dp0.3/season_0/Tibet_with_Multi_seasonalEXP_00004.tfrecord.gz']\n",
            "gs://geebucketwater/m2_Global_Cnn_L8SR_S1A_sl_CCDICE_dp0.3/season_0/Tibet_with_Multi_seasonalEXP_mixer.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 10,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            83.78640553899825,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            31.624111599086813]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 80}\n",
            "Running predictions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de296b78acf141e69d3c49c56c91a1dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Writing patch 12...\n",
            "Writing patch 13...\n",
            "Writing patch 14...\n",
            "Writing patch 15...\n",
            "Writing patch 16...\n",
            "Writing patch 17...\n",
            "Writing patch 18...\n",
            "Writing patch 19...\n",
            "Writing patch 20...\n",
            "Writing patch 21...\n",
            "Writing patch 22...\n",
            "Writing patch 23...\n",
            "Writing patch 24...\n",
            "Writing patch 25...\n",
            "Writing patch 26...\n",
            "Writing patch 27...\n",
            "Writing patch 28...\n",
            "Writing patch 29...\n",
            "Writing patch 30...\n",
            "Writing patch 31...\n",
            "Writing patch 32...\n",
            "Writing patch 33...\n",
            "Writing patch 34...\n",
            "Writing patch 35...\n",
            "Writing patch 36...\n",
            "Writing patch 37...\n",
            "Writing patch 38...\n",
            "Writing patch 39...\n",
            "Writing patch 40...\n",
            "Writing patch 41...\n",
            "Writing patch 42...\n",
            "Writing patch 43...\n",
            "Writing patch 44...\n",
            "Writing patch 45...\n",
            "Writing patch 46...\n",
            "Writing patch 47...\n",
            "Writing patch 48...\n",
            "Writing patch 49...\n",
            "Writing patch 50...\n",
            "Writing patch 51...\n",
            "Writing patch 52...\n",
            "Writing patch 53...\n",
            "Writing patch 54...\n",
            "Writing patch 55...\n",
            "Writing patch 56...\n",
            "Writing patch 57...\n",
            "Writing patch 58...\n",
            "Writing patch 59...\n",
            "Writing patch 60...\n",
            "Writing patch 61...\n",
            "Writing patch 62...\n",
            "Writing patch 63...\n",
            "Writing patch 64...\n",
            "Writing patch 65...\n",
            "Writing patch 66...\n",
            "Writing patch 67...\n",
            "Writing patch 68...\n",
            "Writing patch 69...\n",
            "Writing patch 70...\n",
            "Writing patch 71...\n",
            "Writing patch 72...\n",
            "Writing patch 73...\n",
            "Writing patch 74...\n",
            "Writing patch 75...\n",
            "Writing patch 76...\n",
            "Writing patch 77...\n",
            "Writing patch 78...\n",
            "Writing patch 79...\n",
            "Started upload task with ID: LDXMK67X7XPPGOUUJRJN6NJY\n"
          ]
        }
      ],
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/mewchayutaphong' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# Half this will extend on the sides of each patch.\n",
        "kernel_buffer = [128, 128]\n",
        "\n",
        "tb_region = ee.Geometry.BBox(83.7866460908476, 31.02991423438545, 84.4782964814726, 31.623526673040716)\n",
        "\n",
        "\n",
        "\n",
        "### Loading the model\n",
        "TRAIN_SIZE = 72*10\n",
        "EVAL_SIZE = 72*3\n",
        "configs_multi_global[\"L8SR_S1A_sl_CCDICE_dp0.3\"] = config.configuration(\"L8SR_S1A_sl_CCDICE_dp0.3\", BANDS1 = [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], BANDS2=[\"VV\", \"VH\", \"angle\", \"slope\"]\\\n",
        "                                                 , TRAIN_SIZE= TRAIN_SIZE, EVAL_SIZE = EVAL_SIZE\\\n",
        "                                                 , EPOCHS=10, BATCH_SIZE = 16, dropout_prob=0.3, LOSS=losses_.dice_p_cc, type_ = 2, country=\"Global\")\n",
        "conf = configs_multi_global[\"L8SR_S1A_sl_CCDICE_dp0.3\"]\n",
        "preproc = preprocessing.Preprocessor(conf)\n",
        "MODEL_DIR = 'gs://' + conf.BUCKET + \"/\" + conf.FOLDER + \"/Models/\" + conf.PROJECT_TITLE\n",
        "model_custom = tf.keras.models.load_model(MODEL_DIR, custom_objects={'f1':metrics_.f1, \"dice_p_cc\": losses_.dice_p_cc})\n",
        "\n",
        "### Observing seasonal effect\n",
        "start_dates, end_dates = preprocessing.GenSeasonalDatesMonthly({\"year\": 2018, \"month\": 1}, {\"year\": 2018, \"month\": 12})\n",
        "tb_image_base = f'Tibet_with_Multi_seasonalEXP_'\n",
        "for i in range(len(start_dates)):\n",
        "  S1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "          .filterDate(start_dates[i], end_dates[i]) \\\n",
        "\n",
        "  S1A = S1.median()\n",
        "  l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterDate(start_dates[i], end_dates[i])\n",
        "  L8SR = l8sr.map(preprocessing.maskL8sr).median()\n",
        "  configs_multi_global[\"L8SR_S1A_sl_CCDICE_dp0.3\"].image = ee.Image.cat([L8SR, S1A, slope]).float()\n",
        "  images.doExport(tb_image_base, kernel_buffer, tb_region, conf, \"season_\"+str(i))\n",
        "  out_image_asset, out_image_file, jsonFile = images.doPrediction_multiview_2(tb_image_base, user_folder, \\\n",
        "                          kernel_buffer, model_custom, str(i), conf, \"season_\"+str(i))\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i_W7MsNvUh7"
      },
      "source": [
        "## Objective 4: Flooding Experiment - THWaterNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flooding is common in Thailand due to seasonal monsoon rainfall. The THWaterNet is used to study the flooding extent during dec 2016 to early 2017.\n",
        "\n",
        "Consequently, the result shows that the flood not only affect the built-ups but also cropland and hinders human activies. (More in the paper)"
      ],
      "metadata": {
        "id": "62wm4DDcqr5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "963f39057ed14219a464ebbb60998314",
            "ded034657c134f84a577752f52a22e21",
            "74202a34639946179db7d5eb8e46b0f0",
            "c45a676ad91b4da78188eed074b77db2",
            "75c69a708b2a4a18bf28e478dc6f8c33",
            "d07499f503bd48688f49e6309074f85b",
            "c77f56ac1f574e6ebafc3a541afb9fd1",
            "eb6f5269a738430ead6d32a3273310a0",
            "205800c4b4e34a3d97480affb9760ece",
            "06a05f71e1b941ff9143ee6485780e18",
            "a24136ea99bb499bb84fcaa47b36a4c8",
            "163b85d4b6a844bb95ae9b18d37769e8",
            "3d6a4fbcc2d3468585b9564abf931cdd",
            "925b841589314f0db7605551d286a598",
            "f72772ef7f6240ce86c25724a0232893",
            "dce9552c03a746b89ddb565f68840227",
            "2d81a5ae69e445c18d9b0df598de8d1d",
            "af333ea29fb0413d9187cf90f3a9a066",
            "106df7ddc3be4ad88e1a10f395117dca",
            "582c1d35fe9847219e35630eea933ca3",
            "d76f173e673342379bb3e1ba6fb49da2",
            "834cce7fa94c40c0a277568e0d50d468"
          ]
        },
        "id": "ku6glRIHeNRV",
        "outputId": "6ff62288-5033-47a2-950f-68092217e445"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_291924) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2016-12-01 2016-12-15\n",
            "Running image export to Cloud Storage...\n",
            "Image export completed.\n",
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/m2_TH_Cnn_L8SR_S1A_sl_CC_dp0.3/flood_half0/flood_thai_with_Multi_floodEXP_half_.tfrecord.gz']\n",
            "gs://geebucketwater/m2_TH_Cnn_L8SR_S1A_sl_CC_dp0.3/flood_half0/flood_thai_with_Multi_floodEXP_half_.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 4,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            99.89921729566487,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            7.877057231858849]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 12}\n",
            "Running predictions...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "963f39057ed14219a464ebbb60998314",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Started upload task with ID: W6GYEAALAPUGT5UXNLYELDYT\n",
            "2016-12-15 2016-12-30\n",
            "Running image export to Cloud Storage...\n",
            "Image export completed.\n",
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/m2_TH_Cnn_L8SR_S1A_sl_CC_dp0.3/flood_half1/flood_thai_with_Multi_floodEXP_half_.tfrecord.gz']\n",
            "gs://geebucketwater/m2_TH_Cnn_L8SR_S1A_sl_CC_dp0.3/flood_half1/flood_thai_with_Multi_floodEXP_half_.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 4,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            99.89921729566487,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            7.877057231858849]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 12}\n",
            "Running predictions...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "163b85d4b6a844bb95ae9b18d37769e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Started upload task with ID: GNQYAY4PXXHXGWMAODIFY3XD\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import losses\n",
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/mewchayutaphong' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# Half this will extend on the sides of each patch.\n",
        "kernel_buffer = [128, 128]\n",
        "\n",
        "# tb_region = ee.Geometry.BBox(83.7866460908476, 31.02991423438545, 84.4782964814726, 31.623526673040716)\n",
        "# flood_region = ee.Geometry.BBox(100.0468738016213, 7.634771858293297, 100.1423175272073, 7.749770688005384)\n",
        "flood_region = ee.Geometry.BBox(99.89924501744167, 7.634771858293311, 100.22265505162136, 7.876981296784481)\n",
        "\n",
        "\n",
        "### Loading the model\n",
        "TRAIN_SIZE = 72*10\n",
        "EVAL_SIZE = 72*3\n",
        "configs_multi_global[\"L8SR_S1A_sl_CC_dp0.3\"] = config.configuration(\"L8SR_S1A_sl_CC_dp0.3\", BANDS1 = [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], BANDS2=[\"VV\", \"VH\", \"angle\", \"slope\"]\\\n",
        "                                                 , TRAIN_SIZE= TRAIN_SIZE, EVAL_SIZE = EVAL_SIZE\\\n",
        "                                                 , EPOCHS=10, BATCH_SIZE = 16, dropout_prob=0.3, LOSS=losses.get(\"categorical_crossentropy\"), type_ = 2, country=\"TH\")\n",
        "conf = configs_multi_global[\"L8SR_S1A_sl_CC_dp0.3\"]\n",
        "preproc = preprocessing.Preprocessor(conf)\n",
        "MODEL_DIR = 'gs://' + conf.BUCKET + \"/\" + conf.FOLDER + \"/Models/\" + conf.PROJECT_TITLE\n",
        "model_custom = tf.keras.models.load_model(MODEL_DIR, custom_objects={'f1':metrics_.f1, \"dice_p_cc\": losses_.dice_p_cc})\n",
        "\n",
        "### Observing seasonal effect\n",
        "# start_dates = [\"2016-11-01\", \"2016-12-01\", \"2017-01-01\", \"2017-02-01\", \"2017-03-01\"]\n",
        "# end_dates = [\"2016-12-01\", \"2017-01-01\", \"2017-02-01\", \"2017-03-01\", \"2017-04-01\"]\n",
        "start_dates = [\"2016-12-01\", \"2016-12-15\", \"2017-01-01\", \"2017-01-15\"]\n",
        "end_dates = [\"2016-12-15\", \"2016-12-30\", \"2017-01-15\", \"2017-01-30\"]\n",
        "# start_dates, end_dates = GenSeasonalDatesMonthly({\"year\": 2016, \"month\": 1}, {\"year\": 2018, \"month\": 12})\n",
        "flood_image_base = f'flood_thai_with_Multi_floodEXP_half_'\n",
        "for i in range(len(start_dates)):\n",
        "  if i == 2:\n",
        "    break\n",
        "  print(start_dates[i], end_dates[i])\n",
        "  S1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "          .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "          .filterDate(start_dates[i], end_dates[i]) \\\n",
        "\n",
        "  S1A = S1.median()\n",
        "  l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterDate(start_dates[i], end_dates[i])\n",
        "  L8SR = l8sr.map(preprocessing.maskL8sr).median()\n",
        "  configs_multi_global[\"L8SR_S1A_sl_CC_dp0.3\"].image = ee.Image.cat([L8SR, S1A, slope]).float()\n",
        "  images.doExport(flood_image_base, kernel_buffer, flood_region, conf, \"flood_half\"+str(i))\n",
        "  out_image_asset, out_image_file, jsonFile = images.doPrediction_multiview_2(flood_image_base, user_folder, \\\n",
        "                          kernel_buffer, model_custom, str(i), conf, \"flood_half\"+str(i))\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective 5: GlobalWaterNet - predicting in 10 locations"
      ],
      "metadata": {
        "id": "cvcOOvTVlgoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs_multi_global[\"L8SR_S1A_sl_CC_dp0.3\"] = config.configuration(\"L8SR_S1A_sl_CC_dp0.3\", BANDS1 = [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], BANDS2=[\"VV\", \"VH\", \"angle\", \"slope\"]\\\n",
        "                                                 , TRAIN_SIZE= TRAIN_SIZE, EVAL_SIZE = EVAL_SIZE\\\n",
        "                                                 , EPOCHS=10, BATCH_SIZE = 16, dropout_prob=0.3, LOSS=losses.get(\"categorical_crossentropy\"), type_ = 2, country=\"TH\")\n",
        "conf = configs_multi_global[\"L8SR_S1A_sl_CC_dp0.3\"]\n",
        "preproc = preprocessing.Preprocessor(conf)\n",
        "MODEL_DIR = 'gs://' + conf.BUCKET + \"/\" + conf.FOLDER + \"/Models/\" + conf.PROJECT_TITLE\n",
        "model_custom = tf.keras.models.load_model(MODEL_DIR, custom_objects={'f1':metrics_.f1, \"dice_p_cc\": losses_.dice_p_cc})\n",
        "\n",
        "image_bases = [th_image_base, tb_image_base, gm_image_base, brazil_image_base, mexico_image_base, \\\n",
        "               pakistan_image_base, egypt_image_base, cambodia_image_base, India_image_base, \\\n",
        "               Bangladesh_image_base]\n",
        "\n",
        "images.doExport(th_image_base, kernel_buffer, th_region, conf)\n",
        "images.doExport(tb_image_base, kernel_buffer, tb_region, conf)\n",
        "images.doExport(gm_image_base, kernel_buffer, gm_region, conf)\n",
        "images.doExport(brazil_image_base, kernel_buffer, brazil_region, conf)\n",
        "images.doExport(mexico_image_base, kernel_buffer, mexico_region, conf)\n",
        "images.doExport(pakistan_image_base, kernel_buffer, pakistan_region, conf)\n",
        "images.doExport(egypt_image_base, kernel_buffer, egypt_region, conf)\n",
        "images.doExport(cambodia_image_base, kernel_buffer, cambodia_region, conf)\n",
        "images.doExport(India_image_base, kernel_buffer, India_region, conf)\n",
        "images.doExport(Bangladesh_image_base, kernel_buffer, Bangladesh_region, conf)\n",
        "\n",
        "for image_base in image_bases:\n",
        "  out_image_asset, out_image_file, jsonFile = images.doPrediction_multiview_2(image_base, user_folder, \\\n",
        "                         kernel_buffer, model_custom, \"bestglobal\", conf)\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}"
      ],
      "metadata": {
        "id": "PYehJjkIlfdR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "ImageExport.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06a05f71e1b941ff9143ee6485780e18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106df7ddc3be4ad88e1a10f395117dca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "163b85d4b6a844bb95ae9b18d37769e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d6a4fbcc2d3468585b9564abf931cdd",
              "IPY_MODEL_925b841589314f0db7605551d286a598",
              "IPY_MODEL_f72772ef7f6240ce86c25724a0232893"
            ],
            "layout": "IPY_MODEL_dce9552c03a746b89ddb565f68840227"
          }
        },
        "205800c4b4e34a3d97480affb9760ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d81a5ae69e445c18d9b0df598de8d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6a4fbcc2d3468585b9564abf931cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d81a5ae69e445c18d9b0df598de8d1d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af333ea29fb0413d9187cf90f3a9a066",
            "value": "100%"
          }
        },
        "582c1d35fe9847219e35630eea933ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74202a34639946179db7d5eb8e46b0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6f5269a738430ead6d32a3273310a0",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_205800c4b4e34a3d97480affb9760ece",
            "value": 12
          }
        },
        "75c69a708b2a4a18bf28e478dc6f8c33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834cce7fa94c40c0a277568e0d50d468": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "925b841589314f0db7605551d286a598": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_106df7ddc3be4ad88e1a10f395117dca",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_582c1d35fe9847219e35630eea933ca3",
            "value": 12
          }
        },
        "963f39057ed14219a464ebbb60998314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ded034657c134f84a577752f52a22e21",
              "IPY_MODEL_74202a34639946179db7d5eb8e46b0f0",
              "IPY_MODEL_c45a676ad91b4da78188eed074b77db2"
            ],
            "layout": "IPY_MODEL_75c69a708b2a4a18bf28e478dc6f8c33"
          }
        },
        "a24136ea99bb499bb84fcaa47b36a4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af333ea29fb0413d9187cf90f3a9a066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c45a676ad91b4da78188eed074b77db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a05f71e1b941ff9143ee6485780e18",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a24136ea99bb499bb84fcaa47b36a4c8",
            "value": " 12/12 [00:33&lt;00:00,  2.76s/it]"
          }
        },
        "c77f56ac1f574e6ebafc3a541afb9fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d07499f503bd48688f49e6309074f85b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76f173e673342379bb3e1ba6fb49da2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce9552c03a746b89ddb565f68840227": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded034657c134f84a577752f52a22e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d07499f503bd48688f49e6309074f85b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c77f56ac1f574e6ebafc3a541afb9fd1",
            "value": "100%"
          }
        },
        "eb6f5269a738430ead6d32a3273310a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72772ef7f6240ce86c25724a0232893": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76f173e673342379bb3e1ba6fb49da2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_834cce7fa94c40c0a277568e0d50d468",
            "value": " 12/12 [00:33&lt;00:00,  2.74s/it]"
          }
        },
        "90b185d3e825481ea8cc84e57f319311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afa131ca49b8443db40c9565c1af84f9",
              "IPY_MODEL_1b05f18f1c2f47d2a80f58c5cccf1810",
              "IPY_MODEL_c1053db7dd0a4790b376dd325f46fcf1"
            ],
            "layout": "IPY_MODEL_1df70e234c1a4a019f1c9e3e25b914f1"
          }
        },
        "afa131ca49b8443db40c9565c1af84f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65dcfad174d24a7b9113bed5d8acb76c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c0aea30ee1c74e14ba1c906972ab868b",
            "value": "100%"
          }
        },
        "1b05f18f1c2f47d2a80f58c5cccf1810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f49fb692664cf58219fedf1403c6d6",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cd316bcd0b44a1c9eab350875724ad9",
            "value": 42
          }
        },
        "c1053db7dd0a4790b376dd325f46fcf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a92aae7c5d56463e8e81c10c27716359",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_78825933fce54de3ab323fef574d9c69",
            "value": " 42/42 [04:13&lt;00:00,  2.81s/it]"
          }
        },
        "1df70e234c1a4a019f1c9e3e25b914f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65dcfad174d24a7b9113bed5d8acb76c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0aea30ee1c74e14ba1c906972ab868b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77f49fb692664cf58219fedf1403c6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd316bcd0b44a1c9eab350875724ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a92aae7c5d56463e8e81c10c27716359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78825933fce54de3ab323fef574d9c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de296b78acf141e69d3c49c56c91a1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9fba35ec9a9465b99dd342f00dbc678",
              "IPY_MODEL_6b94a353054b40b6a3708394906e4c84",
              "IPY_MODEL_38d007102f5743d4846a443fb7b9959d"
            ],
            "layout": "IPY_MODEL_5bb788bb08114a90bfc6b9f2c83ddf6c"
          }
        },
        "d9fba35ec9a9465b99dd342f00dbc678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44a9ca7c913144de8f5d76ce07e4e457",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4a85beda500e4902a8a10433d3d47582",
            "value": "100%"
          }
        },
        "6b94a353054b40b6a3708394906e4c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c11996d2ff364e34901acbf581bc5e04",
            "max": 80,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b3ccd236e940cb92ca279463f6dbc2",
            "value": 80
          }
        },
        "38d007102f5743d4846a443fb7b9959d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaad54d3075a499893c9b2eebb717179",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_64eef717599b4b3f957fb4de68f4e034",
            "value": " 80/80 [03:46&lt;00:00,  2.80s/it]"
          }
        },
        "5bb788bb08114a90bfc6b9f2c83ddf6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a9ca7c913144de8f5d76ce07e4e457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a85beda500e4902a8a10433d3d47582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c11996d2ff364e34901acbf581bc5e04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b3ccd236e940cb92ca279463f6dbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaad54d3075a499893c9b2eebb717179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64eef717599b4b3f957fb4de68f4e034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}