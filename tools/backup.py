
# import json
# from pprint import pprint
# import numpy as np
# import tqdm.notebook as tq


# def doExport(out_image_base, kernel_buffer, region, setting, extra_folder= ""):
#   """Run the image export task.  Block until complete.
#   """
#   if extra_folder == "":
#     fileNamePrefix_ = setting.FOLDER + '/' + out_image_base
#   else:
#     fileNamePrefix_ = setting.FOLDER + '/' + extra_folder + '/' + out_image_base
#   task = ee.batch.Export.image.toCloudStorage(
#     image = setting.image.select(setting.BANDS),
#     description = out_image_base,
#     bucket = setting.BUCKET,
#     fileNamePrefix = fileNamePrefix_,
#     region = region.getInfo()['coordinates'],
#     scale = 30,
#     fileFormat = 'TFRecord',
#     maxPixels = 1e10,
#     formatOptions = {
#       'patchDimensions': setting.KERNEL_SHAPE,
#       'kernelSize': kernel_buffer,
#       'compressed': True,
#       'maxFileSize': 104857600
#     }
#   )
#   task.start()

#   # Block until the task completes.
#   print('Running image export to Cloud Storage...')
#   import time
#   while task.active():
#     time.sleep(30)

#   # Error condition
#   if task.status()['state'] != 'COMPLETED':
#     print('Error with image export.')
#   else:
#     print('Image export completed.')

# def LoadImage(out_image_base, user_folder, kernel_buffer, setting, extra_folder = ""):
#   print('Looking for TFRecord files...')

#   # Get a list of all the files in the output bucket.
#   if extra_folder == "":
#     filesList = !gsutil ls 'gs://'{setting.BUCKET}'/'{setting.FOLDER}
#   else:
#     filesList = !gsutil ls 'gs://'{setting.BUCKET}'/'{setting.FOLDER}'/'{extra_folder}

#   # Get only the files generated by the image export.
#   exportFilesList = [s for s in filesList if out_image_base in s]

#   # Get the list of image files and the JSON mixer file.
#   imageFilesList = []
#   jsonFile = None
#   for f in exportFilesList:
#     if f.endswith('.tfrecord.gz'):
#       imageFilesList.append(f)
#     elif f.endswith('.json'):
#       jsonFile = f

#   # Make sure the files are in the right order.
#   imageFilesList.sort()

#   pprint(imageFilesList)
#   print(jsonFile)

#   # Load the contents of the mixer file to a JSON object.
#   jsonText = !gsutil cat {jsonFile}
#   # Get a single string w/ newlines from the IPython.utils.text.SList
#   mixer = json.loads(jsonText.nlstr)
#   pprint(mixer)
#   patches = mixer['totalPatches']

#   # Get set up for prediction.
#   x_buffer = int(kernel_buffer[0] / 2)
#   y_buffer = int(kernel_buffer[1] / 2)

#   buffered_shape = [
#       setting.KERNEL_SHAPE[0] + kernel_buffer[0],
#       setting.KERNEL_SHAPE[1] + kernel_buffer[1]]

#   imageColumns = [
#     tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) 
#       for k in setting.BANDS
#   ]

#   imageFeaturesDict = dict(zip(setting.BANDS, imageColumns))

#   def parse_image(example_proto):
#     return tf.io.parse_single_example(example_proto, imageFeaturesDict)

#   def toTupleImage(inputs):
#     inputsList = [inputs.get(key) for key in setting.BANDS]
#     stacked = tf.stack(inputsList, axis=0)
#     stacked = tf.transpose(stacked, [1, 2, 0])
#     return stacked

#    # Create a dataset from the TFRecord file(s) in Cloud Storage.
#   imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')
#   imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)
#   imageDataset = imageDataset.map(toTupleImage).batch(1)
#   return imageDataset, patches, x_buffer, y_buffer, jsonFile

# def predictionSingleinput(model, imageDataset, patches):
#   print('Running predictions...')
#   predictions = model.predict(imageDataset, steps=patches, verbose=1)
#   return predictions

# def predictionMultipleinput(model, imageDataset, patches, setting):
#   print('Running predictions...')
#   predictions = []
#   pbar = tq.tqdm(total=patches)
#   for data in imageDataset:
#     pbar.update(1)
#     x1, x2 = tf.split(data, [len(setting.BANDS1), len(setting.BANDS2)], 3)
#     predictions.append(model.predict([x1, x2], verbose=0))
#   return predictions

# def predictionMultipleinput_3(model, imageDataset, patches, setting):
#   print('Running predictions...')
#   predictions = []
#   pbar = tq.tqdm(total=patches)
#   for data in imageDataset:
#     pbar.update(1)
#     x1, x2, x3 = tf.split(data, [len(setting.BANDS1), len(setting.BANDS2), len(setting.BANDS3)], 3)
#     predictions.append(model.predict([x1, x2, x3], verbose=0))
#   return predictions

# def uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, multiview=False, user_folder = 'users/mewchayutaphong'):
#   print('Writing predictions...')
#   out_image_file = 'gs://' + setting.BUCKET + '/' + setting.FOLDER + '/' + out_image_base + '.TFRecord'
#   writer = tf.io.TFRecordWriter(out_image_file)
#   patches = 0
#   for predictionPatch in predictions:
#     if multiview == True:
#       predictionPatch = predictionPatch[0]
#     print('Writing patch ' + str(patches) + '...')
#     predictionPatch = predictionPatch[
#         x_buffer:x_buffer+setting.KERNEL_SIZE, y_buffer:y_buffer+setting.KERNEL_SIZE]
#     predictionPatch = np.argmax(predictionPatch, -1)
#     example = tf.train.Example(
#       features=tf.train.Features(
#         feature={
#           'prediction': tf.train.Feature(
#               float_list=tf.train.FloatList(
#                   value=predictionPatch.flatten()))
#         }
#       )
#     )
#     # Write the example.
#     writer.write(example.SerializeToString())
#     patches += 1

#   writer.close()

#   # Start the upload.
#   out_image_asset = user_folder + '/' + out_image_base + suffix
#   !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}

# def doPrediction_featurestack(out_image_base, user_folder, kernel_buffer, model, suffix, setting):
#   """Perform inference on exported imagery, upload to Earth Engine.
#   """
#   imageDataset, patches, x_buffer, y_buffer, jsonFile = LoadImage(out_image_base, user_folder, kernel_buffer, setting)
#   predictions = predictionSingleinput(model, imageDataset, patches)
#   uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, False)
#   return

# def doPrediction_multiview_2(out_image_base, user_folder, kernel_buffer, model, suffix, setting, extra_folder=""):
#   """Perform inference on exported imagery, upload to Earth Engine.
#   """
#   imageDataset, patches, x_buffer, y_buffer, jsonFile = LoadImage(out_image_base, user_folder, kernel_buffer, setting, extra_folder)
#   predictions = predictionMultipleinput(model, imageDataset, patches, setting)
#   uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, True)
#   return

# def doPrediction_multiview_3(out_image_base, user_folder, kernel_buffer, model, suffix, setting):
#   """Perform inference on exported imagery, upload to Earth Engine.
#   """
#   imageDataset, patches, x_buffer, y_buffer, jsonFile = LoadImage(out_image_base, user_folder, kernel_buffer, setting)
#   predictions = predictionMultipleinput_3(model, imageDataset, patches, setting)
#   uploadToGEEAsset(x_buffer, y_buffer, predictions, out_image_base, jsonFile, suffix, setting, True)
#   return