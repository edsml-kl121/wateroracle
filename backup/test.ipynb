{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is an Earth Engine <> TensorFlow demonstration notebook.  Suppose you want to predict a continuous output (regression) from a stack of continuous inputs.  In this example, the output is impervious surface area from [NLCD](https://www.mrlc.gov/data) and the input is a Landsat 8 composite.  The model is a [fully convolutional neural network (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597). This notebook shows:\n",
        "\n",
        "1.   Exporting training/testing patches from Earth Engine, suitable for training an FCNN model.\n",
        "2.   Preprocessing.\n",
        "3.   Training and validating an FCNN model.\n",
        "4.   Making predictions with the trained model and importing them to Earth Engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8ycmzClYwf"
      },
      "source": [
        "# Variables\n",
        "\n",
        "Declare the variables that will be in use throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqj2CqknpH4W",
        "outputId": "5c919420-5c13-4919-fe78-dfcf9916a2bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Jul  6 13:22 sample_data\n",
            "total 0\n",
            "-rw-r--r-- 1 root root 0 Jul  9 07:06 __init__.py\n"
          ]
        }
      ],
      "source": [
        "PACKAGE_PATH = 'Water_classification_package'\n",
        "\n",
        "!ls -l\n",
        "!mkdir {PACKAGE_PATH}\n",
        "!touch {PACKAGE_PATH}/__init__.py\n",
        "!ls -l {PACKAGE_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSjF1WoiovwA",
        "outputId": "0bfc0311-394c-4978-96b4-a8dd4bcd3756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Water_classification_package/metrics.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/metrics.py\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "\n",
        "# Acc = TP + TN / (TP + TN + FP + FN)\n",
        "# possible_pos = TP + FN\n",
        "# predicted_pos = TP + FP\n",
        "# Missing TN\n",
        "# TN = total - possible_pos - predicted_pos + TP\n",
        "# TP + TN + FP + FN = possible_pos + predicted_pos - TP + TN\n",
        "\n",
        "def custom_accuracy(y_true, y_pred):\n",
        "    # total_data = K.int_shape(y_true) + K.int_shape(y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    true_negatives = K.sum(K.round(K.clip(1 - y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    total_data = - true_positives + true_negatives + possible_positives + predicted_positives\n",
        "  \n",
        "    # total_data = tf.cast(total_data, tf.float32|tf.int32)\n",
        "    # true_positives = tf.cast(true_positives, tf.float32|tf.int32)\n",
        "    # possible_positives = tf.cast(possible_positives, tf.float32|tf.int32)\n",
        "    # predicted_positives = tf.cast(predicted_positives, tf.float32|tf.int32)\n",
        "    # print(K.int_shape(y_true), K.int_shape(y_pred))\n",
        "    # print(K.int_shape(y_pred)[0], K.int_shape(y_pred)[1], K.int_shape(y_pred)[2])\n",
        "    # print(total_data)\n",
        "    # print(possible_positives)\n",
        "    # (true_positives) / (total_data + K.epsilon())\n",
        "    return (true_positives + true_negatives) / (total_data + K.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGiBufMkoys3",
        "outputId": "6ade82fc-7be7-4b7c-dc04-c5851af50ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "\n",
        "project_id = 'coastal-cell-299117'\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N24qKm5UXjnd",
        "outputId": "af65b1cb-ac1c-40a5-b635-fa1b2d831c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Water_classification_package/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/config.py\n",
        "\n",
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "from . import metrics\n",
        "\n",
        "# INSERT YOUR BUCKET HERE:\n",
        "BUCKET = 'geebucketwater'\n",
        "\n",
        "\n",
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'Cnn_S1_DEM'\n",
        "TRAINING_BASE = 'training_patches_S1_DEM'\n",
        "EVAL_BASE = 'eval_patches_S1_DEM'\n",
        "TEST_BASE_1 = 'test_patches_S1_DEM_1'\n",
        "TEST_BASE_2 = 'test_patches_S1_DEM_2'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "BANDS = ['VV', \"VH\", \"angle\", \"slope\", \"aspect\", \"elevation\"]\n",
        "# thermalBands = ['B10', 'B11']\n",
        "RESPONSE = 'water'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 16000\n",
        "EVAL_SIZE = 8000\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 1\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS = 'categorical_crossentropy'\n",
        "METRICS = ['AUC', metrics.f1, metrics.custom_accuracy]\n",
        "# METRICS = ['RootMeanSquaredError']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN561FWLxIxX",
        "outputId": "6544ed61-1a43-48f3-e115-196288e155c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Water_classification_package/preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/preprocessing.py\n",
        "\n",
        "from . import config\n",
        "import tensorflow as tf\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, config.FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in config.FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(config.BANDS)], tf.reshape(tf.one_hot(tf.cast(stacked[:,:,len(config.BANDS):], tf.int32), depth=2),[256,256,2])\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + config.BUCKET + '/' + config.FOLDER + '/' + config.TRAINING_BASE + '*'\n",
        "\tprint(glob)\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.shuffle(config.BUFFER_SIZE).batch(config.BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "\n",
        "# print(iter(training.take(1)).next())\n",
        "\n",
        "def get_eval_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + config.BUCKET + '/' + config.FOLDER + '/' + config.EVAL_BASE + '*'\n",
        "\tprint(glob)\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "# print(iter(evaluation.take(1)).next())\n",
        "\n",
        "def get_test_dataset(folder):\n",
        "  \"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "  glob = 'gs://' + config.BUCKET + '/' + config.FOLDER + '/' + folder + '*'\n",
        "  print(glob)\n",
        "  dataset = get_dataset(glob)\n",
        "  dataset = dataset.batch(1).repeat()\n",
        "  return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twfMI-eKrEC-",
        "outputId": "d0ba6c27-7063-4544-8ba0-93a19c986c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Water_classification_package/visualizer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PACKAGE_PATH}/visualizer.py\n",
        "import folium\n",
        "def mapping(image, bandName, min, max, name):\n",
        "  mapid = image.getMapId({'bands': [bandName], 'min': min, \"max\": max})\n",
        "  map = folium.Map(location=[38., -122.5])\n",
        "  folium.TileLayer(\n",
        "      tiles=mapid['tile_fetcher'].url_format,\n",
        "      attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "      overlay=True,\n",
        "      name=name,\n",
        "    ).add_to(map)\n",
        "\n",
        "  map.add_child(folium.LayerControl())\n",
        "  return map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pZIfsiRyqko"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Authenticate and import as necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dqQ2Vmxpv28",
        "outputId": "d590a29f-7d46-4e0d-b4e5-5ff75c637b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=RndpjXhoOGyUixUjcY4xNBn2k7RN_r7moxuqAbUlXM8&tc=tM9F8c8WRs9rIa1I4EysGYcToSumVXQpNGRqjBnC1M8&cc=8nvD4rGTkGnq1ZCBd5a0Vt1ZXnXuNDJ4Sfl5UqWzjk8\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/1AdQt8qjAS_dQOlDWpQHHmv3CZxHoLBVU9XL6PheGyGhtLKOKzj4wWLQbPeI\n",
            "\n",
            "Successfully saved authorization token.\n",
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from Water_classification_package import config, visualizer, metrics, preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Imagery\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 8 composite.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "outputs": [],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "S1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filterDate('2018-01-01','2018-02-01') \\\n",
        "        # .filterDate('2017-01-01','2018-02-01')\n",
        "\n",
        "dem = ee.Image('NASA/NASADEM_HGT/001').select('elevation')\n",
        "slope = ee.Terrain.slope(dem)\n",
        "aspect = ee.Terrain.aspect(dem)\n",
        "image = ee.Image.cat([\n",
        "  S1.median(),\n",
        "  dem,\n",
        "  slope,\n",
        "  aspect\n",
        "]).float()\n",
        "\n",
        "waterdata = ee.ImageCollection('JRC/GSW1_3/MonthlyHistory').filterDate('2018-01-01', '2018-02-01').median()\n",
        "watermask = waterdata.select(\"water\")\n",
        "mask = watermask.gt(0)\n",
        "maskedComposite = waterdata.updateMask(mask).subtract(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Stack the 2D images (Landsat composite and NLCD impervious surface) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG7T8zvRmyqV",
        "outputId": "b429d28a-83f0-4847-9867-e4b087532b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'type': 'Image', 'bands': [{'id': 'VV', 'data_type': {'type': 'PixelType', 'precision': 'float', 'dimensions': 2}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}, {'id': 'VH', 'data_type': {'type': 'PixelType', 'precision': 'float', 'dimensions': 2}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}, {'id': 'angle', 'data_type': {'type': 'PixelType', 'precision': 'float', 'dimensions': 2}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}, {'id': 'slope', 'data_type': {'type': 'PixelType', 'precision': 'float', 'dimensions': 2}, 'crs': 'EPSG:4326', 'crs_transform': [0.0002777777777777778, 0, -179.0001388888889, 0, -0.0002777777777777778, 61.00013888888889]}, {'id': 'aspect', 'data_type': {'type': 'PixelType', 'precision': 'float', 'dimensions': 2}, 'crs': 'EPSG:4326', 'crs_transform': [0.0002777777777777778, 0, -179.0001388888889, 0, -0.0002777777777777778, 61.00013888888889]}, {'id': 'elevation', 'data_type': {'type': 'PixelType', 'precision': 'float', 'dimensions': 2}, 'dimensions': [1288801, 421201], 'crs': 'EPSG:4326', 'crs_transform': [0.0002777777777777778, 0, -179.0001388888889, 0, -0.0002777777777777778, 61.00013888888889]}, {'id': 'water', 'data_type': {'type': 'PixelType', 'precision': 'float', 'dimensions': 2}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}]}\n"
          ]
        }
      ],
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(config.BANDS),\n",
        "  maskedComposite.select(config.RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, config.KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, config.KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(config.KERNEL_SIZE, config.KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)\n",
        "print(arrays.getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "ure_WaD0itQY",
        "outputId": "06ed51f9-fcfc-4d3d-8b99-65638f599bc1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;head&gt;    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    &lt;script&gt;L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css&quot;/&gt;\n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    \n",
              "    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "        initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "    &lt;style&gt;#map_1d68e81a051de53d4a63e35e9bda4bc0 {\n",
              "        position: relative;\n",
              "        width: 100.0%;\n",
              "        height: 100.0%;\n",
              "        left: 0.0%;\n",
              "        top: 0.0%;\n",
              "        }\n",
              "    &lt;/style&gt;\n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;    \n",
              "    \n",
              "    &lt;div class=&quot;folium-map&quot; id=&quot;map_1d68e81a051de53d4a63e35e9bda4bc0&quot; &gt;&lt;/div&gt;\n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;    \n",
              "    \n",
              "    \n",
              "        var bounds = null;\n",
              "    \n",
              "\n",
              "    var map_1d68e81a051de53d4a63e35e9bda4bc0 = L.map(\n",
              "        &#x27;map_1d68e81a051de53d4a63e35e9bda4bc0&#x27;, {\n",
              "        center: [16.426385350573945, 102.16833965985882],\n",
              "        zoom: 5,\n",
              "        maxBounds: bounds,\n",
              "        layers: [],\n",
              "        worldCopyJump: false,\n",
              "        crs: L.CRS.EPSG3857,\n",
              "        zoomControl: true,\n",
              "        });\n",
              "\n",
              "\n",
              "    \n",
              "    var tile_layer_1f01b36dc0d046c5990bf871ac9207be = L.tileLayer(\n",
              "        &#x27;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#x27;,\n",
              "        {\n",
              "        &quot;attribution&quot;: null,\n",
              "        &quot;detectRetina&quot;: false,\n",
              "        &quot;maxNativeZoom&quot;: 18,\n",
              "        &quot;maxZoom&quot;: 18,\n",
              "        &quot;minZoom&quot;: 0,\n",
              "        &quot;noWrap&quot;: false,\n",
              "        &quot;opacity&quot;: 1,\n",
              "        &quot;subdomains&quot;: &quot;abc&quot;,\n",
              "        &quot;tms&quot;: false\n",
              "}).addTo(map_1d68e81a051de53d4a63e35e9bda4bc0);\n",
              "    var tile_layer_8fd5dee1e69e62b42a3a7bba83869296 = L.tileLayer(\n",
              "        &#x27;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/b38379c61d56bdb22d03f0128e9d7bdd-f372f6e6c6b3e1d5825e6ea2264f9a7f/tiles/{z}/{x}/{y}&#x27;,\n",
              "        {\n",
              "        &quot;attribution&quot;: &quot;Map Data &amp;copy; &lt;a href=\\&quot;https://earthengine.google.com/\\&quot;&gt;Google Earth Engine&lt;/a&gt;&quot;,\n",
              "        &quot;detectRetina&quot;: false,\n",
              "        &quot;maxNativeZoom&quot;: 18,\n",
              "        &quot;maxZoom&quot;: 18,\n",
              "        &quot;minZoom&quot;: 0,\n",
              "        &quot;noWrap&quot;: false,\n",
              "        &quot;opacity&quot;: 1,\n",
              "        &quot;subdomains&quot;: &quot;abc&quot;,\n",
              "        &quot;tms&quot;: false\n",
              "}).addTo(map_1d68e81a051de53d4a63e35e9bda4bc0);\n",
              "    \n",
              "            var layer_control_1488ad1406f7fa8afa2d53e61b98133c = {\n",
              "                base_layers : { &quot;openstreetmap&quot; : tile_layer_1f01b36dc0d046c5990bf871ac9207be, },\n",
              "                overlays : { &quot;training polygons&quot; : tile_layer_8fd5dee1e69e62b42a3a7bba83869296, }\n",
              "                };\n",
              "            L.control.layers(\n",
              "                layer_control_1488ad1406f7fa8afa2d53e61b98133c.base_layers,\n",
              "                layer_control_1488ad1406f7fa8afa2d53e61b98133c.overlays,\n",
              "                {position: &#x27;topright&#x27;,\n",
              "                 collapsed: true,\n",
              "                 autoZIndex: true\n",
              "                }).addTo(map_1d68e81a051de53d4a63e35e9bda4bc0);\n",
              "            \n",
              "        \n",
              "&lt;/script&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f74e2bd4b10>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import folium\n",
        "# trainingPolys = ee.FeatureCollection('projects/google/DemoTrainingGeometries')\n",
        "trainingPolys = ee.FeatureCollection('users/mewchayutaphong/thailandTraining')\n",
        "# evalPolys = ee.FeatureCollection('projects/google/DemoEvalGeometries')\n",
        "first = ee.Geometry.BBox(101.78381817548382, 14.052178100305664, 102.27820294110882, 14.361037359593043);\n",
        "second = ee.Geometry.BBox(102.16833965985882, 16.426385350573945, 102.83850567548382, 16.921030330473783);\n",
        "evalPolys = ee.FeatureCollection(first).merge(second)\n",
        "testPolys1 = ee.FeatureCollection(ee.Geometry.BBox(106.42682423644103, 15.001360239504, 106.8014457430141, 15.59647593973863))\n",
        "testPolys2 = ee.FeatureCollection(ee.Geometry.BBox(70.3090276140532, 31.351437257990685, 71.5724553484282, 32.32209892418571))\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2).paint(testPolys1, 3).paint(testPolys2, 4)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 3, 'palette': ['red', 'blue', \"green\", \"purple\"]})\n",
        "map = folium.Map(location=[16.426385350573945, 102.16833965985882], zoom_start=5)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do NOT need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A"
      },
      "outputs": [],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "evalPolysList = evalPolys.toList(evalPolys.size())\n",
        "\n",
        "# These numbers were determined experimentally.\n",
        "n = 200 # Number of shards in each polygon.\n",
        "N = 2000 # Total sample size in each polygon.\n",
        "\n",
        "# Export all the training data (in many pieces), ith one task \n",
        "# per geometry.\n",
        "for g in range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    # geomSample = geomSample.filter(ee.Filter.greaterThan('water', -1))\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = config.TRAINING_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = config.BUCKET,\n",
        "    fileNamePrefix = config.FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = config.BANDS + [config.RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    # geomSample = geomSample.filter(ee.Filter.greaterThan('water', -1))\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = config.FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = config.BANDS + [config.RESPONSE]\n",
        "  )\n",
        "  task.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfNl3nnXpITf"
      },
      "outputs": [],
      "source": [
        "testPolys1List = testPolys1.toList(testPolys1.size())\n",
        "testPolys2List = testPolys2.toList(testPolys2.size())\n",
        "\n",
        "\n",
        "n = 200 # Number of shards in each polygon.\n",
        "N = 2000 # Total sample size in each polygon.\n",
        "\n",
        "# Export all the test data.\n",
        "for g in range(testPolys1.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(testPolys1List.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    # geomSample = geomSample.filter(ee.Filter.greaterThan('water', -1))\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = config.TEST_BASE_1 + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = config.BUCKET,\n",
        "    fileNamePrefix = config.FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = config.BANDS + [config.RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "# Export all the test2 data.\n",
        "for g in range(testPolys2.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(testPolys2List.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    # geomSample = geomSample.filter(ee.Filter.greaterThan('water', -1))\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = config.TEST_BASE_2 + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = config.BUCKET,\n",
        "    fileNamePrefix = config.FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = config.BANDS + [config.RESPONSE]\n",
        "  )\n",
        "  task.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j85EiibkBS29",
        "outputId": "e1e7e50f-23e1-49c2-b5ba-fabd7d376d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Task DZNT7JENLFVDQXLEGHEKBIIH EXPORT_FEATURES: test_patches_S1_DEM_2_g0 (COMPLETED)>\n",
            "<Task ZCGNLXKJ5AKTDRKYU3V5W7DL EXPORT_FEATURES: test_patches_S1_DEM_1_g0 (COMPLETED)>\n",
            "<Task N4DBOSE3EPTELVECZ3YM4DEP EXPORT_FEATURES: eval_patches_S1_DEM_g1 (COMPLETED)>\n"
          ]
        }
      ],
      "source": [
        "# Print all tasks.\n",
        "from pprint import pprint\n",
        "# pprint(ee.batch.Task.list())\n",
        "for i in range(3):\n",
        "  pprint(ee.batch.Task.list()[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmlWQVZna_M",
        "outputId": "18713e4a-605f-48b2-cfc8-736be72ca348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Canceling task \"ZR7VW6COKRPDX66PDFMAWXF5\"\n",
            "Canceling task \"X62KTRH4SZZLJTGPKZSKRDM4\"\n",
            "Canceling task \"LKASP242HNQOWMYRSPFZDG7M\"\n",
            "Canceling task \"OVXYIUXWHIOTFNDA7C6UCRNN\"\n",
            "Canceling task \"JMJMK2MM7IBKUKGTSO5KLHDC\"\n",
            "Canceling task \"QCCKC6NPOG2EUZLDI2FJ73I3\"\n"
          ]
        }
      ],
      "source": [
        "!earthengine task cancel all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Training data\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm0qRF0fAYcC",
        "outputId": "7701d9e1-1809-4fe2-a223-ba50f6afdcc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://geebucketwater/Cnn_S1_DEM/training_patches_S1_DEM*\n",
            "gs://geebucketwater/Cnn_S1_DEM/eval_patches_S1_DEM*\n",
            "gs://geebucketwater/Cnn_S1_DEM/test_patches_S1_DEM_1*\n",
            "gs://geebucketwater/Cnn_S1_DEM/test_patches_S1_DEM_2*\n"
          ]
        }
      ],
      "source": [
        "training = preprocessing.get_training_dataset()\n",
        "evaluation = preprocessing.get_eval_dataset()\n",
        "test_1 = preprocessing.get_test_dataset(config.TEST_BASE_1)\n",
        "test_2 = preprocessing.get_test_dataset(config.TEST_BASE_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Model\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model.  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem.  Since impervious surface fraction is constrained to [0,1], with many values close to zero or one, a saturating activation function is suitable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "# from tensorflow.python.keras.layers.experimental.preprocessing.Normalization import BatchNormalization\n",
        "\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):\n",
        "    conv = layers.Conv2D(n_filters, \n",
        "                  3,  # filter size\n",
        "                  activation='relu',\n",
        "                  padding='same',\n",
        "                  kernel_initializer='HeNormal')(inputs)\n",
        "    conv = layers.Conv2D(n_filters, \n",
        "                  3,  # filter size\n",
        "                  activation='relu',\n",
        "                  padding='same',\n",
        "                  kernel_initializer='HeNormal')(conv)\n",
        "  \n",
        "    conv = layers.BatchNormalization()(conv, training=False)\n",
        "    if dropout_prob > 0:     \n",
        "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
        "    if max_pooling:\n",
        "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
        "    else:\n",
        "        next_layer = conv\n",
        "    skip_connection = conv    \n",
        "    return next_layer, skip_connection\n",
        "\n",
        "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
        "    up = layers.Conv2DTranspose(\n",
        "                 n_filters,\n",
        "                 (3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='same')(prev_layer_input)\n",
        "    merge = layers.concatenate([up, skip_layer_input], axis=3)\n",
        "    conv = layers.Conv2D(n_filters, \n",
        "                 3,  \n",
        "                 activation='relu',\n",
        "                 padding='same',\n",
        "                 kernel_initializer='HeNormal')(merge)\n",
        "    conv = layers.Conv2D(n_filters,\n",
        "                 3, \n",
        "                 activation='relu',\n",
        "                 padding='same',\n",
        "                 kernel_initializer='HeNormal')(conv)\n",
        "    return conv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RsY9GOyuDvmp"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = EncoderMiniBlock(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = EncoderMiniBlock(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = EncoderMiniBlock(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = EncoderMiniBlock(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = EncoderMiniBlock(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = DecoderMiniBlock(center, encoder4, 512) # 16\n",
        "\tdecoder3 = DecoderMiniBlock(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = DecoderMiniBlock(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = DecoderMiniBlock(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = DecoderMiniBlock(decoder1, encoder0, 32) # 256\n",
        "\t# outputs = layers.Conv2D(3, (1, 1), activation='softmax')(decoder0)\n",
        "\t# outputs = layers.Conv2D(3, (1, 1), activation='softmax')(decoder0)\n",
        "\toutputs = layers.Dense(2, activation=tf.nn.softmax)(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER), \n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HYmrCnfxqn9"
      },
      "source": [
        "# Landsat Training from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiAgJ5tNAZxP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TD7sQUVbOGs"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    print(\"hi\")\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(float(train_acc_metric.result()))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, tf.math.argmax(val_logits))\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "# Instantiate a loss function.\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "Train_values = []\n",
        "Val_values = []\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = tf.keras.metrics.AUC()\n",
        "val_acc_metric = tf.keras.metrics.AUC()\n",
        "train_acc_metric.reset_states()\n",
        "\n",
        "model = get_model()\n",
        "import time\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(evaluation):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "        print(step)\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    # print(\"Training acc over epoch: %.4f\" % (float(train_acc),),)\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in evaluation:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "    Train_values.append(float(train_acc))\n",
        "    Val_values.append(float(val_acc))\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),), \"Validation acc: %.4f\" % (float(val_acc),), \"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "    # print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    # print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w_dI43v3b1q"
      },
      "outputs": [],
      "source": [
        "print(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS2b3s9qQIWk"
      },
      "outputs": [],
      "source": [
        "model.save(\"/\")\n",
        "model.save_weights('./checkpoints/my_checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZWJ_dcBL2BB"
      },
      "outputs": [],
      "source": [
        "model.load_weights('./checkpoints/my_checkpoint')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWW6JFFmx2tz"
      },
      "source": [
        "# Custom Keras training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "axJqBSYmyPYy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "class CustomModel(tf.keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "        # print(x.numpy())\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self(x, training=False)\n",
        "        # Updates the metrics tracking the loss\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "# Define model here\n",
        "inputs = layers.Input(shape=[None, None, len(config.BANDS)]) # 256\n",
        "encoder0_pool, encoder0 = EncoderMiniBlock(inputs, 32) # 128\n",
        "encoder1_pool, encoder1 = EncoderMiniBlock(encoder0_pool, 64) # 64\n",
        "encoder2_pool, encoder2 = EncoderMiniBlock(encoder1_pool, 128) # 32\n",
        "encoder3_pool, encoder3 = EncoderMiniBlock(encoder2_pool, 256) # 16\n",
        "encoder4_pool, encoder4 = EncoderMiniBlock(encoder3_pool, 512) # 8\n",
        "center = conv_block(encoder4_pool, 1024) # center\n",
        "decoder4 = DecoderMiniBlock(center, encoder4, 512) # 16\n",
        "decoder3 = DecoderMiniBlock(decoder4, encoder3, 256) # 32\n",
        "decoder2 = DecoderMiniBlock(decoder3, encoder2, 128) # 64\n",
        "decoder1 = DecoderMiniBlock(decoder2, encoder1, 64) # 128\n",
        "decoder0 = DecoderMiniBlock(decoder1, encoder0, 32) # 256\n",
        "# outputs = layers.Conv2D(3, (1, 1), activation='softmax')(decoder0)\n",
        "# outputs = layers.Conv2D(3, (1, 1), activation='softmax')(decoder0)\n",
        "outputs = layers.Dense(2, activation=tf.nn.softmax)(decoder0)\n",
        "\n",
        "model_custom = CustomModel(inputs, outputs)\n",
        "\n",
        "model_custom.compile(\n",
        "  optimizer=optimizers.get(config.OPTIMIZER), \n",
        "  loss=losses.get(config.LOSS),\n",
        "  metrics=[config.METRICS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F40VmNZjzAXw"
      },
      "outputs": [],
      "source": [
        "for EPOCHS in range(1,4):\n",
        "  EPO = [i for i in range(1, EPOCHS + 1)]\n",
        "  model_custom = CustomModel(inputs, outputs)\n",
        "  model_custom.compile(\n",
        "  optimizer=optimizers.get(config.OPTIMIZER), \n",
        "  loss=losses.get(config.LOSS),\n",
        "  metrics=[config.METRICS])\n",
        "  history = model_custom.fit(\n",
        "      x=training,\n",
        "      epochs=EPOCHS,\n",
        "      steps_per_epoch=int(2000*3 / config.BATCH_SIZE),\n",
        "      validation_data=evaluation,\n",
        "      validation_steps=2000*2)\n",
        "\n",
        "  Model_name = \"S1_DEM\" + \"_EPOCHS_\" + str(config.EPOCHS)\n",
        "  MODEL_DIR = 'gs://' + config.BUCKET + \"/\" + config.FOLDER + \"/Models/\" + Model_name\n",
        "  model_custom.save(config.MODEL_DIR, save_format='tf')\n",
        "  hist_keys = [*history.history]\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
        "  axes[0].plot(EPO, history.history[hist_keys[0]])\n",
        "  axes[0].plot(EPO, history.history[hist_keys[2]])\n",
        "  axes[0].legend(['Loss', 'Val_loss'])\n",
        "  axes[1].plot(EPO, history.history[hist_keys[1]])\n",
        "  axes[1].plot(EPO, history.history[hist_keys[3]])\n",
        "  axes[1].legend(['AUC', 'Val_AUC'])\n",
        "  fig.savefig(f'epoch{EPOCHS}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smFQQmw0-Rml",
        "outputId": "0311bbd9-c573-4798-ef44-5a6405c4d095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading...\n",
            "loading...\n",
            "loading...\n"
          ]
        }
      ],
      "source": [
        "MODEL_DIR_1 = 'gs://' + config.BUCKET + \"/\" + config.FOLDER + \"/Models/S1CNNmodel_EPOCHS_\" + str(1)\n",
        "MODEL_DIR_2 = 'gs://' + config.BUCKET + \"/\" + config.FOLDER + \"/Models/S1CNNmodel_EPOCHS_\" + str(2)\n",
        "MODEL_DIR_3 = 'gs://' + config.BUCKET + \"/\" + config.FOLDER + \"/Models/S1CNNmodel_EPOCHS_\" + str(3)\n",
        "\n",
        "print(\"loading...\", MODEL_DIR_1)\n",
        "model_custom_1EP = tf.keras.models.load_model(MODEL_DIR_1)\n",
        "print(\"loading...\", MODEL_DIR_1)\n",
        "model_custom_2EP = tf.keras.models.load_model(MODEL_DIR_2)\n",
        "print(\"loading...\", MODEL_DIR_1)\n",
        "model_custom_3EP = tf.keras.models.load_model(MODEL_DIR_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Automatic Fit keras Training\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "\n",
        "model.fit(\n",
        "    x=evaluation,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE),\n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XrwZHp66j4"
      },
      "source": [
        "Note that the notebook VM is sometimes not heavy-duty enough to get through a whole training job, especially if you have a large buffer size or a large number of epochs.  You can still use this notebook for training, but may need to set up an alternative VM ([learn more](https://research.google.com/colaboratory/local-runtimes.html)) for production use.  Alternatively, you can package your code for running large training jobs on Google's AI Platform [as described here](https://cloud.google.com/ml-engine/docs/tensorflow/trainer-considerations).  The following code loads a pre-trained model, which you can use for predictions right away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RJpNfEUS1qp"
      },
      "outputs": [],
      "source": [
        "# # Load a trained model. 50 epochs. 25 hours. Final RMSE ~0.08.\n",
        "# MODEL_DIR = 'gs://ee-docs-demos/fcnn-demo/trainer/model'\n",
        "# m = tf.keras.models.load_model(MODEL_DIR)\n",
        "# m.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storge bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n",
        "\n",
        "The following functions handle this process.  It's useful to separate the export from the predictions so that you can experiment with different models without running the export every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + out_image_base,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to Cloud Storage...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B63fcYUaelWC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def doPrediction(out_image_base, user_folder, kernel_buffer, region, model, suffix):\n",
        "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # Get a list of all the files in the output bucket.\n",
        "  filesList = !gsutil ls 'gs://'{BUCKET}'/'{FOLDER}\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  jsonText = !gsutil cat {jsonFile}\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) \n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = model.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + '/' + out_image_base + '.TFRecord'\n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "    predictionPatch = np.argmax(predictionPatch, -1)\n",
        "    # print(predictionPatch)\n",
        "    # print(predictionPatch.shape)\n",
        "    # print(\"pred\", tf.argmax(predictionPatch, 1))\n",
        "    # print(predictionPatch[0][0])\n",
        "    # print(predictionPatch[0][1])\n",
        "    # print(predictionPatch[0][2])\n",
        "    \n",
        "#     patch[0].append(tf.argmax(predictionPatch, 1))\n",
        "#     patch[1].append(predictionPatch[0][0])\n",
        "#     patch[2].append(predictionPatch[0][1])\n",
        "#     patch[3].append(predictionPatch[0][2])\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'prediction': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=predictionPatch.flatten()))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  writer.close()\n",
        "\n",
        "  # Start the upload.\n",
        "  out_image_asset = user_folder + '/' + out_image_base + suffix\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqlymOehnQO"
      },
      "source": [
        "Now there's all the code needed to run the prediction pipeline, all that remains is to specify the output region in which to do the prediction, the names of the output files, where to put them, and the shape of the outputs.  In terms of the shape, the model is trained on 256x256 patches, but can work (in theory) on any patch that's big enough with even dimensions ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)).  Because of tile boundary artifacts, give the model slightly larger patches for prediction, then clip out the middle 256x256 patch.  This is controlled with a kernel buffer, half the size of which will extend beyond the kernel buffer.  For example, specifying a 128x128 kernel will append 64 pixels on each side of the patch, to ensure that the pixels in the output are taken from inputs completely covered by the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPANwc7B1-TS"
      },
      "outputs": [],
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/mewchayutaphong' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# Half this will extend on the sides of each patch.\n",
        "kernel_buffer = [128, 128]\n",
        "bj_kernel_buffer = [128, 128]\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "bj_image_base = 'FCNN_demo_beijing_384_S1'\n",
        "# Beijing\n",
        "bj_region = ee.Geometry.Polygon(\n",
        "        [[[115.9662455210937, 40.121362012835235],\n",
        "          [115.9662455210937, 39.64293313749715],\n",
        "          [117.01818643906245, 39.64293313749715],\n",
        "          [117.01818643906245, 40.121362012835235]]], None, False)\n",
        "\n",
        "th_image_base = \"Thai_water_with_S1\"\n",
        "th_region = ee.Geometry.BBox(106.42682423644103, 15.001360239504, 106.8014457430141, 15.59647593973863)\n",
        "\n",
        "\n",
        "np_image_base = \"Nepal_water_with_S1\"\n",
        "np_region = ee.Geometry.BBox(70.3090276140532, 31.351437257990685, 71.5724553484282, 32.32209892418571)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "outputs": [],
      "source": [
        "# Run the export.\n",
        "# doExport(bj_image_base, bj_kernel_buffer, bj_region)\n",
        "doExport(np_image_base, kernel_buffer, np_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sus_KOngCUyw",
        "outputId": "454b2102-a3ad-4846-9b21-384d25728c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running image export to Cloud Storage...\n",
            "Image export completed.\n"
          ]
        }
      ],
      "source": [
        "doExport(th_image_base, kernel_buffer, th_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7gpHJGLCMgf",
        "outputId": "d09e65f4-c234-4491-8e08-38ef6c242d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/fcnn-demo_S1/Thai_water_with_S1.tfrecord.gz']\n",
            "gs://geebucketwater/fcnn-demo_S1/Thai_water_with_S1.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 5,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            106.42664564466256,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            15.596729625939957]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 40}\n",
            "Running predictions...\n",
            "40/40 [==============================] - 9s 150ms/step\n",
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Writing patch 12...\n",
            "Writing patch 13...\n",
            "Writing patch 14...\n",
            "Writing patch 15...\n",
            "Writing patch 16...\n",
            "Writing patch 17...\n",
            "Writing patch 18...\n",
            "Writing patch 19...\n",
            "Writing patch 20...\n",
            "Writing patch 21...\n",
            "Writing patch 22...\n",
            "Writing patch 23...\n",
            "Writing patch 24...\n",
            "Writing patch 25...\n",
            "Writing patch 26...\n",
            "Writing patch 27...\n",
            "Writing patch 28...\n",
            "Writing patch 29...\n",
            "Writing patch 30...\n",
            "Writing patch 31...\n",
            "Writing patch 32...\n",
            "Writing patch 33...\n",
            "Writing patch 34...\n",
            "Writing patch 35...\n",
            "Writing patch 36...\n",
            "Writing patch 37...\n",
            "Writing patch 38...\n",
            "Writing patch 39...\n",
            "Started upload task with ID: LOGMGQHCH5GIKPI3FITFFRXZ\n",
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/fcnn-demo_S1/Thai_water_with_S1.tfrecord.gz']\n",
            "gs://geebucketwater/fcnn-demo_S1/Thai_water_with_S1.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 5,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            106.42664564466256,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            15.596729625939957]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 40}\n",
            "Running predictions...\n",
            "40/40 [==============================] - 6s 68ms/step\n",
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Writing patch 12...\n",
            "Writing patch 13...\n",
            "Writing patch 14...\n",
            "Writing patch 15...\n",
            "Writing patch 16...\n",
            "Writing patch 17...\n",
            "Writing patch 18...\n",
            "Writing patch 19...\n",
            "Writing patch 20...\n",
            "Writing patch 21...\n",
            "Writing patch 22...\n",
            "Writing patch 23...\n",
            "Writing patch 24...\n",
            "Writing patch 25...\n",
            "Writing patch 26...\n",
            "Writing patch 27...\n",
            "Writing patch 28...\n",
            "Writing patch 29...\n",
            "Writing patch 30...\n",
            "Writing patch 31...\n",
            "Writing patch 32...\n",
            "Writing patch 33...\n",
            "Writing patch 34...\n",
            "Writing patch 35...\n",
            "Writing patch 36...\n",
            "Writing patch 37...\n",
            "Writing patch 38...\n",
            "Writing patch 39...\n",
            "Started upload task with ID: YERISQR4FZYUU7VE4EER66HF\n",
            "Looking for TFRecord files...\n",
            "['gs://geebucketwater/fcnn-demo_S1/Thai_water_with_S1.tfrecord.gz']\n",
            "gs://geebucketwater/fcnn-demo_S1/Thai_water_with_S1.json\n",
            "{'patchDimensions': [256, 256],\n",
            " 'patchesPerRow': 5,\n",
            " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
            "                                            0.0,\n",
            "                                            106.42664564466256,\n",
            "                                            0.0,\n",
            "                                            -0.00026949458523585647,\n",
            "                                            15.596729625939957]},\n",
            "                'crs': 'EPSG:4326'},\n",
            " 'totalPatches': 40}\n",
            "Running predictions...\n",
            "40/40 [==============================] - 4s 88ms/step\n",
            "Writing predictions...\n",
            "Writing patch 0...\n",
            "Writing patch 1...\n",
            "Writing patch 2...\n",
            "Writing patch 3...\n",
            "Writing patch 4...\n",
            "Writing patch 5...\n",
            "Writing patch 6...\n",
            "Writing patch 7...\n",
            "Writing patch 8...\n",
            "Writing patch 9...\n",
            "Writing patch 10...\n",
            "Writing patch 11...\n",
            "Writing patch 12...\n",
            "Writing patch 13...\n",
            "Writing patch 14...\n",
            "Writing patch 15...\n",
            "Writing patch 16...\n",
            "Writing patch 17...\n",
            "Writing patch 18...\n",
            "Writing patch 19...\n",
            "Writing patch 20...\n",
            "Writing patch 21...\n",
            "Writing patch 22...\n",
            "Writing patch 23...\n",
            "Writing patch 24...\n",
            "Writing patch 25...\n",
            "Writing patch 26...\n",
            "Writing patch 27...\n",
            "Writing patch 28...\n",
            "Writing patch 29...\n",
            "Writing patch 30...\n",
            "Writing patch 31...\n",
            "Writing patch 32...\n",
            "Writing patch 33...\n",
            "Writing patch 34...\n",
            "Writing patch 35...\n",
            "Writing patch 36...\n",
            "Writing patch 37...\n",
            "Writing patch 38...\n",
            "Writing patch 39...\n",
            "Started upload task with ID: W4WCSAZV2LCHMEGLPKC2QW3J\n"
          ]
        }
      ],
      "source": [
        "doPrediction(th_image_base, user_folder, kernel_buffer, th_region, model_custom_1EP, \"_epochs_1_\")\n",
        "doPrediction(th_image_base, user_folder, kernel_buffer, th_region, model_custom_2EP, \"_epochs_2_\")\n",
        "doPrediction(th_image_base, user_folder, kernel_buffer, th_region, model_custom_3EP, \"_epochs_3_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "outputs": [],
      "source": [
        "# Run the prediction.\n",
        "# doPrediction(bj_image_base, user_folder, bj_kernel_buffer, bj_region)\n",
        "doPrediction(np_image_base, user_folder, kernel_buffer, np_region, model_custom_1EP, \"_epochs_1\")\n",
        "doPrediction(np_image_base, user_folder, kernel_buffer, np_region, model_custom_2EP, \"_epochs_2\")\n",
        "doPrediction(np_image_base, user_folder, kernel_buffer, np_region, model_custom_3EP, \"_epochs_3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoOE1zgOxvEm"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CExbhfEd2JvH"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "import tqdm.notebook as tq\n",
        "import numpy as np\n",
        "\n",
        "def MetricCalculator(model, test_data, total_steps):\n",
        "  TP = 0\n",
        "  TN = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "  # total_steps = 2000\n",
        "  test_acc_metric = tf.keras.metrics.Accuracy()\n",
        "  # test_F1_metric = tfa.metrics.F1Score(num_classes=2, threshold=0.5)\n",
        "  pbar = tq.tqdm(total=total_steps)\n",
        "  for steps, data in enumerate(test_data):\n",
        "    # print(f'Number of steps: {steps}', end = \"\\r\")\n",
        "    pbar.update(1)\n",
        "    if steps == total_steps:\n",
        "      break\n",
        "    input = data[0]\n",
        "    y_true = data[1]\n",
        "    y_pred = np.rint(model.predict(input))\n",
        "    y_true = np.reshape(y_true, (256*256,2))\n",
        "    y_pred = np.reshape(y_pred, (256*256,2))\n",
        "    # print(y_pred[0][1] == 1, y_pred[0][1] == 1)\n",
        "    for j in range(y_pred.shape[0]):\n",
        "      if y_true[j][1] == 1 and y_pred[j][1] == 1:\n",
        "        TP += 1\n",
        "      if y_true[j][1] == 0 and y_pred[j][1] == 0:\n",
        "        TN += 1\n",
        "      if y_true[j][1] == 1 and y_pred[j][1] == 0:\n",
        "        FN += 1\n",
        "      if y_true[j][1] == 0 and y_pred[j][1] == 1:\n",
        "        FP += 1\n",
        "    test_acc_metric.update_state(y_true, y_pred)\n",
        "    # # recall_metric.update_state(data[1], test_logits)\n",
        "    # # precision_metric.update_state(data[1], test_logits)\n",
        "\n",
        "  print(test_acc_metric.result().numpy())\n",
        "  print(\"TP: \", TP)\n",
        "  print(\"TN: \", TN)\n",
        "  print(\"FP: \", FP)\n",
        "  print(\"FN: \", FN)\n",
        "  precision = TP/(TP + FP)\n",
        "  recall = TP/(TP + FN)\n",
        "\n",
        "  print(\"precision\", precision)\n",
        "  print(\"recall\", recall)\n",
        "\n",
        "  F1 = 2*(recall*precision)/(recall + precision)\n",
        "  print(F1)\n",
        "\n",
        "  print(TP, TN, FP, FN)\n",
        "  return F1, test_acc_metric.result().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3a4a4417962d40399ec1d680929044f0",
            "a554c9d4a225455db2c51a39776fc85a",
            "96150daef51c40eb9e9216b39b39d1c7",
            "e5e1f96a905e4e80ba783daeba40a486",
            "49405d2e590541ac8c0cbba97b2fe362",
            "071cebde55314239bf235d07e909991b",
            "49ba2a60ed2d43d8b52c060d23321375",
            "5251282b5163415ca13082bc14128d19",
            "9be07f8c68584521815181669ca9b3b3",
            "9c576b5acba048c7a506c553a4525c48",
            "fcbdf24a09cb43bba0e9acb64b959eae"
          ]
        },
        "id": "1ualwqxkFykC",
        "outputId": "4a5aa2fd-5be5-4872-8cf8-559b294695c7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a4a4417962d40399ec1d680929044f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.99617374\n",
            "TP:  1516497\n",
            "TN:  129053939\n",
            "FP:  48497\n",
            "FN:  453067\n",
            "precision 0.9690113827912439\n",
            "recall 0.7699658401554862\n",
            "0.8580971086059416\n",
            "1516497 129053939 48497 453067\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8580971086059416, 0.99617374)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MetricCalculator(model_custom_1EP, test_1, 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "fbfbfa96e8c84187b9995e6176013784"
          ]
        },
        "id": "z5oNy2k5FzAH",
        "outputId": "7e627563-80ea-476f-da90-29ff36227be1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbfbfa96e8c84187b9995e6176013784",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9940117\n",
            "TP:  1194074\n",
            "TN:  129092936\n",
            "FP:  9500\n",
            "FN:  775490\n",
            "precision 0.9921068417895368\n",
            "recall 0.6062631120390096\n",
            "0.7526139739273867\n",
            "1194074 129092936 9500 775490\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7526139739273867, 0.9940117)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MetricCalculator(model_custom_2EP, test_1, 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "ddb4eb5856534756adc97767b7239373"
          ]
        },
        "id": "DaGkhmeuFzNZ",
        "outputId": "dece8f70-76a1-49d9-9783-e3fc43080465"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddb4eb5856534756adc97767b7239373",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.99250096\n",
            "TP:  987610\n",
            "TN:  129101303\n",
            "FP:  1133\n",
            "FN:  981954\n",
            "precision 0.9988541006105732\n",
            "recall 0.5014358507771263\n",
            "0.6676859433452985\n",
            "987610 129101303 1133 981954\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6676859433452985, 0.99250096)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MetricCalculator(model_custom_3EP, test_1, 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXqxbhIIF0f1"
      },
      "outputs": [],
      "source": [
        "MetricCalculator(model_custom_1EP, test_2, 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcuULcbVF0mi"
      },
      "outputs": [],
      "source": [
        "MetricCalculator(model_custom_2EP, test_2, 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nl9Ga1WF0r6"
      },
      "outputs": [],
      "source": [
        "MetricCalculator(model_custom_3EP, test_2, 2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "# Display the output\n",
        "\n",
        "One the data has been exported, the model has made predictions and the predictions have been written to a file, and the image imported to Earth Engine, it's possible to display the resultant Earth Engine asset.  Here, display the impervious area predictions over Beijing, China."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zISF9pKa0c3f"
      },
      "outputs": [],
      "source": [
        "user_folder + '/' + bj_image_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgco6HJ4R5p2"
      },
      "outputs": [],
      "source": [
        "out_image = ee.Image(user_folder + '/' + bj_image_base)\n",
        "mapid = out_image.getMapId({'min': 0, 'max': 2})\n",
        "map = folium.Map(location=[39.898, 116.5097])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted impervious',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20399ifV_PV0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "9JIE7Yl87lgU",
        "_HYmrCnfxqn9",
        "uu_E7OTDBCoS"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "S1+DEM+CNNfromScratch",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "071cebde55314239bf235d07e909991b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a4a4417962d40399ec1d680929044f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a554c9d4a225455db2c51a39776fc85a",
              "IPY_MODEL_96150daef51c40eb9e9216b39b39d1c7",
              "IPY_MODEL_e5e1f96a905e4e80ba783daeba40a486"
            ],
            "layout": "IPY_MODEL_49405d2e590541ac8c0cbba97b2fe362"
          }
        },
        "49405d2e590541ac8c0cbba97b2fe362": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ba2a60ed2d43d8b52c060d23321375": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5251282b5163415ca13082bc14128d19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96150daef51c40eb9e9216b39b39d1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5251282b5163415ca13082bc14128d19",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9be07f8c68584521815181669ca9b3b3",
            "value": 1960
          }
        },
        "9be07f8c68584521815181669ca9b3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c576b5acba048c7a506c553a4525c48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a554c9d4a225455db2c51a39776fc85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071cebde55314239bf235d07e909991b",
            "placeholder": "​",
            "style": "IPY_MODEL_49ba2a60ed2d43d8b52c060d23321375",
            "value": " 98%"
          }
        },
        "e5e1f96a905e4e80ba783daeba40a486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c576b5acba048c7a506c553a4525c48",
            "placeholder": "​",
            "style": "IPY_MODEL_fcbdf24a09cb43bba0e9acb64b959eae",
            "value": " 1960/2000 [24:44&lt;00:30,  1.33it/s]"
          }
        },
        "fcbdf24a09cb43bba0e9acb64b959eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
